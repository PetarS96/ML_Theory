{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DQN Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN)\n",
    "\n",
    "---\n",
    "\n",
    "## Theory\n",
    "Deep Q-Network (DQN) is a model-free, off-policy reinforcement learning algorithm that combines Q-learning with deep neural networks. It is designed to learn the optimal action-selection policy for an agent interacting with an environment by approximating the Q-value function using a deep neural network. DQN is particularly effective for environments with large state-action spaces where traditional Q-learning would be infeasible.\n",
    "\n",
    "The main idea is to:\n",
    "- Use a deep neural network to approximate the Q-value function.\n",
    "- Utilize experience replay to store and sample past experiences.\n",
    "- Employ a target network to stabilize the training process.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundation\n",
    "- **Q-value Function**:\n",
    "  The Q-value function \\( Q(s, a) \\) represents the expected cumulative reward of taking action \\( a \\) in state \\( s \\) and following the optimal policy thereafter.\n",
    "\n",
    "- **Bellman Equation**:\n",
    "  The Q-value is updated using the Bellman equation:\n",
    "  $$ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right) $$\n",
    "  where:\n",
    "  - \\( s \\) is the current state.\n",
    "  - \\( a \\) is the current action.\n",
    "  - \\( r \\) is the reward received after taking action \\( a \\).\n",
    "  - \\( s' \\) is the next state.\n",
    "  - \\( \\alpha \\) is the learning rate (0 < \\( \\alpha \\) ≤ 1).\n",
    "  - \\( \\gamma \\) is the discount factor (0 ≤ \\( \\gamma \\) ≤ 1).\n",
    "\n",
    "- **Deep Neural Network**:\n",
    "  A neural network is used to approximate the Q-value function. The network takes the state as input and outputs Q-values for all possible actions.\n",
    "\n",
    "- **Experience Replay**:\n",
    "  A replay buffer is used to store past experiences (state, action, reward, next state, done). During training, random samples from the buffer are used to update the Q-network, breaking the correlation between consecutive experiences and improving training stability.\n",
    "\n",
    "- **Target Network**:\n",
    "  A separate target network is used to compute the target Q-values. The target network is periodically updated to match the weights of the main Q-network, reducing the risk of divergence and improving training stability.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm Steps\n",
    "1. **Initialize Q-network and Target Network**:\n",
    "   - Initialize the Q-network with random weights.\n",
    "   - Initialize the target network with the same weights as the Q-network.\n",
    "\n",
    "2. **Initialize Replay Buffer**:\n",
    "   - Create a replay buffer to store past experiences.\n",
    "\n",
    "3. **Repeat for each episode**:\n",
    "   - Initialize the starting state.\n",
    "\n",
    "4. **Repeat for each step of the episode**:\n",
    "   - Choose an action \\( a \\) based on the exploration strategy (e.g., \\( \\epsilon \\)-greedy).\n",
    "   - Take action \\( a \\), observe the reward \\( r \\) and the next state \\( s' \\).\n",
    "   - Store the experience (state, action, reward, next state, done) in the replay buffer.\n",
    "   - Sample a mini-batch of experiences from the replay buffer.\n",
    "   - Compute the target Q-value for each experience using the target network:\n",
    "     $$ y = r + \\gamma \\max_{a'} Q_{\\text{target}}(s', a') $$\n",
    "   - Update the Q-network by minimizing the loss between the predicted Q-value and the target Q-value:\n",
    "     $$ L = \\left( y - Q(s, a) \\right)^2 $$\n",
    "   - Periodically update the target network to match the Q-network.\n",
    "\n",
    "5. **End of Episode**:\n",
    "   - If the episode ends (e.g., the agent reaches a terminal state), reset the environment.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components\n",
    "- **Q-network**: A neural network that approximates the Q-value function.\n",
    "- **Target Network**: A copy of the Q-network used to compute target Q-values.\n",
    "- **Replay Buffer**: A buffer that stores past experiences for training.\n",
    "- **Exploration Strategy**: A strategy to balance exploration and exploitation (e.g., \\( \\epsilon \\)-greedy).\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "- Effective for environments with large state-action spaces.\n",
    "- Utilizes deep learning to approximate the Q-value function.\n",
    "- Stabilizes training using experience replay and target networks.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages\n",
    "- Requires significant computational resources for training.\n",
    "- Can be unstable if hyperparameters are not tuned properly.\n",
    "- Sensitive to the choice of neural network architecture and learning parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Tips\n",
    "- Use **experience replay** to improve sample efficiency and training stability.\n",
    "- Employ **target networks** to reduce the risk of divergence and stabilize training.\n",
    "- Use **exploration strategies** like \\( \\epsilon \\)-greedy or decayed \\( \\epsilon \\)-greedy to balance exploration and exploitation.\n",
    "- Normalize rewards to ensure stable learning.\n",
    "- Regularly monitor and adjust hyperparameters to optimize performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "- Game playing (e.g., Atari games)\n",
    "- Robotics (e.g., navigation, manipulation)\n",
    "- Autonomous driving\n",
    "- Financial trading\n",
    "- Industrial automation\n",
    "\n",
    "Deep Q-Network (DQN) is a powerful reinforcement learning algorithm that leverages deep learning to handle complex environments with large state-action spaces. Its ability to learn from raw sensory input makes it a valuable tool for many real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation for Deep Q-Network (DQN)\n",
    "\n",
    "### 1. Q-values\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the learned Q-values to ensure they converge to the optimal values.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher Q-values indicate better expected rewards for the corresponding state-action pairs.\n",
    "- Compare Q-values across different episodes to observe convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Cumulative Reward\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Cumulative Reward} = \\sum_{t=0}^T r_t\n",
    "$$\n",
    "where:\n",
    "- \\( r_t \\) is the reward received at time step \\( t \\).\n",
    "- \\( T \\) is the total number of time steps.\n",
    "\n",
    "**Description:**\n",
    "- Measures the total reward accumulated by the agent over an episode.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher cumulative rewards indicate better performance.\n",
    "- Plot cumulative rewards across episodes to observe learning progress.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Convergence\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the convergence of the Q-values and cumulative rewards over time.\n",
    "\n",
    "**Interpretation:**\n",
    "- Convergence indicates that the agent has learned a stable policy.\n",
    "- Plot Q-values and cumulative rewards across episodes to check for convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Policy Evaluation\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the learned policy by observing the actions taken by the agent.\n",
    "\n",
    "**Interpretation:**\n",
    "- A good policy should result in optimal actions being taken in each state.\n",
    "- Compare the learned policy with the optimal policy (if known) to assess performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Exploration vs. Exploitation\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the balance between exploration and exploitation during training.\n",
    "\n",
    "**Interpretation:**\n",
    "- A good balance ensures that the agent explores the environment sufficiently while also exploiting the learned policy.\n",
    "- Plot the exploration rate (\\(\\epsilon\\)) over time to observe the balance.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Learning Rate (\\(\\alpha\\))\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the impact of the learning rate on the agent's performance.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher learning rate may lead to faster convergence but can cause instability.\n",
    "- A lower learning rate may result in slower convergence but more stable learning.\n",
    "- Experiment with different learning rates to find the optimal value.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Discount Factor (\\(\\gamma\\))\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the impact of the discount factor on the agent's performance.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher discount factor places more importance on future rewards.\n",
    "- A lower discount factor places more importance on immediate rewards.\n",
    "- Experiment with different discount factors to find the optimal value.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Training Time\n",
    "\n",
    "**Description:**\n",
    "- Measure the time taken to train the agent.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower training times indicate more efficient learning.\n",
    "- Evaluate training time in conjunction with other metrics to assess overall performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Success Rate\n",
    "\n",
    "**Description:**\n",
    "- Measure the success rate of the agent in achieving its goal.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher success rates indicate better performance.\n",
    "- Plot success rates across episodes to observe learning progress.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Episode Length\n",
    "\n",
    "**Description:**\n",
    "- Measure the average length of episodes.\n",
    "\n",
    "**Interpretation:**\n",
    "- Shorter episodes may indicate better performance (e.g., reaching the goal faster).\n",
    "- Plot episode lengths across episodes to observe learning progress.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network (DQN)\n",
    "\n",
    "### TensorFlow/Keras Implementation\n",
    "\n",
    "| **Parameter**          | **Description**                                                                 |\n",
    "|------------------------|-------------------------------------------------------------------------------|\n",
    "| state_size             | The size of the state space.                                                   |\n",
    "| action_size            | The size of the action space.                                                  |\n",
    "| learning_rate          | The learning rate for the Q-network.                                           |\n",
    "| gamma                  | Discount factor for future rewards (0 ≤ gamma ≤ 1).                            |\n",
    "| epsilon                | Exploration rate for the epsilon-greedy policy.                                |\n",
    "| epsilon_decay          | Decay rate for the exploration rate.                                           |\n",
    "| epsilon_min            | Minimum value of epsilon.                                                      |\n",
    "| batch_size             | Size of the mini-batch used for training.                                       |\n",
    "| memory_capacity        | Capacity of the replay buffer.                                                 |\n",
    "| target_update_freq     | Frequency of updating the target network.                                      |\n",
    "\n",
    "---\n",
    "\n",
    "| **Attribute**          | **Description**                                                                 |\n",
    "|------------------------|-------------------------------------------------------------------------------|\n",
    "| q_network              | Neural network model to approximate the Q-value function.                      |\n",
    "| target_network         | Neural network model to provide stable target Q-values.                        |\n",
    "| replay_buffer          | Buffer to store past experiences for experience replay.                        |\n",
    "\n",
    "---\n",
    "\n",
    "| **Method**             | **Description**                                                                 |\n",
    "|------------------------|-------------------------------------------------------------------------------|\n",
    "| build_model()          | Build the Q-network and target network models.                                 |\n",
    "| remember(state, action, reward, next_state, done) | Store experiences in the replay buffer.                |\n",
    "| choose_action(state)   | Choose an action based on the epsilon-greedy policy.                           |\n",
    "| replay()               | Train the Q-network using mini-batches from the replay buffer.                 |\n",
    "| update_target_network()| Update the weights of the target network to match the Q-network.               |\n",
    "| train(env, num_episodes)| Train the DQN agent in the given environment.                                  |\n",
    "| evaluate(env, num_episodes)| Evaluate the performance of the DQN agent in the environment.               |\n",
    "\n",
    "---\n",
    "\n",
    "| **Usage Example (Python)**  | **Code**                                                                 |\n",
    "|-----------------------------|------------------------------------------------------------------------|\n",
    "| **Initialize DQN agent**    | `agent = DQNAgent(state_size, action_size, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, batch_size=64, memory_capacity=100000, target_update_freq=10)` |\n",
    "| **Train the agent**         | `agent.train(env, num_episodes=1000)`                                   |\n",
    "| **Evaluate the agent**      | `agent.evaluate(env, num_episodes=100)`                                 |\n",
    "| **Choose an action**        | `action = agent.choose_action(state)`                                   |\n",
    "| **Store experiences**       | `agent.remember(state, action, reward, next_state, done)`               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXX regression - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-10 08:28:30.245283: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-10 08:28:30.529704: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-10 08:28:30.792507: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739176111.038985   18018 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739176111.106314   18018 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-10 08:28:31.657131: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/petar-ubuntu/Learning/ML_Theory/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-02-10 08:28:39.383889: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Initialize and train the DQN agent\u001b[39;00m\n\u001b[1;32m    113\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQNAgent(state_size, action_size)\n\u001b[0;32m--> 114\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Data Plotting\u001b[39;00m\n\u001b[1;32m    117\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mrange\u001b[39m(num_episodes), rewards)\n",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[1;32m     73\u001b[0m     state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 74\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):\n",
      "File \u001b[0;32m~/Learning/ML_Theory/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:299\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/ML_Theory/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, method, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/Learning/ML_Theory/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:42\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrapit\u001b[39m(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m---> 42\u001b[0m     conv \u001b[38;5;241m=\u001b[39m \u001b[43m_array_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# As this already tried the method, subok is maybe quite reasonable here\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# but this follows what was done before. TODO: revisit this.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     arr, \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mas_arrays(subok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Loading\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "batch_size = 64\n",
    "memory_capacity = 100000\n",
    "target_update_freq = 10\n",
    "num_episodes = 1000\n",
    "\n",
    "# Model Definition\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_capacity)\n",
    "        self.q_network = self.build_model()\n",
    "        self.target_network = self.build_model()\n",
    "        self.update_target_network()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build_model(self):\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(layers.Dense(24, activation='relu'))\n",
    "        model.add(layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(optimizer=optimizers.Adam(learning_rate=learning_rate), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.q_network.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target += gamma * np.amax(self.target_network.predict(next_state)[0])\n",
    "            target_f = self.q_network.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.q_network.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > epsilon_min:\n",
    "            self.epsilon *= epsilon_decay\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def train(self):\n",
    "        rewards = []\n",
    "        for e in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            total_rewards = 0\n",
    "            for time in range(500):\n",
    "                action = self.choose_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_rewards += reward\n",
    "                if done:\n",
    "                    self.update_target_network()\n",
    "                    print(f\"Episode: {e+1}/{num_episodes}, Score: {time}, Epsilon: {self.epsilon:.2}\")\n",
    "                    break\n",
    "                self.replay()\n",
    "                if e % target_update_freq == 0:\n",
    "                    self.update_target_network()\n",
    "            rewards.append(total_rewards)\n",
    "        return rewards\n",
    "\n",
    "    def evaluate(self, num_eval_episodes=100):\n",
    "        total_eval_rewards = 0\n",
    "        for e in range(num_eval_episodes):\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "            episode_rewards = 0\n",
    "            for time in range(500):\n",
    "                action = np.argmax(self.q_network.predict(state)[0])\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                episode_rewards += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "            total_eval_rewards += episode_rewards\n",
    "        avg_eval_reward = total_eval_rewards / num_eval_episodes\n",
    "        print(f\"Average Evaluation Reward: {avg_eval_reward}\")\n",
    "        return avg_eval_reward\n",
    "\n",
    "# Initialize and train the DQN agent\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "rewards = agent.train()\n",
    "\n",
    "# Data Plotting\n",
    "plt.plot(range(num_episodes), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Training Rewards over Episodes')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the DQN agent\n",
    "agent.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
