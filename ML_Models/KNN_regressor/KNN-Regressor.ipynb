{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **KNN Regressor Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "K-Nearest Neighbors (KNN) Regressor is a simple, non-parametric algorithm that makes predictions based on the average of the target values of the nearest neighbors in the feature space. For a given input point, the algorithm identifies the `K` closest data points in the training set and predicts the target variable as the mean (or weighted mean) of their corresponding target values.\n",
    "\n",
    "The model function for KNN Regressor is:\n",
    "\n",
    "$$ f(x) = \\frac{1}{K} \\sum_{i=1}^{K} y_i $$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ is the predicted output.\n",
    "- $K$ is the number of nearest neighbors considered for the prediction.\n",
    "- $y_i$ is the target value of the $i$-th nearest neighbor.\n",
    "\n",
    "The main idea behind KNN is that similar data points (in terms of feature similarity) are likely to have similar target values.\n",
    "\n",
    "## Model Training\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "The forward pass in KNN is simple: given a new input point $x$, the algorithm calculates the distances between $x$ and all points in the training set. The K closest training points are selected, and their target values are averaged (or weighted, if using weighted KNN) to produce the prediction.\n",
    "\n",
    "### Distance Metric\n",
    "\n",
    "The performance of KNN heavily depends on the choice of distance metric used to measure similarity between points. Common distance metrics include:\n",
    "- **Euclidean Distance**: $ d(x_1, x_2) = \\sqrt{\\sum_{i=1}^{n} (x_{1i} - x_{2i})^2} $\n",
    "- **Manhattan Distance**: $ d(x_1, x_2) = \\sum_{i=1}^{n} |x_{1i} - x_{2i}| $\n",
    "- **Minkowski Distance**: A generalization of both Euclidean and Manhattan distances.\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "KNN does not have an explicit cost function or model parameters like other regression algorithms (e.g., linear regression or SVM). Instead, it makes predictions by evaluating the similarity between data points. The model relies on the concept of neighbors to calculate predictions, and the error is typically measured using a performance metric like Mean Squared Error (MSE) after making predictions.\n",
    "\n",
    "The prediction error is given by:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^{(i)}) - y^{(i)})^2 $$\n",
    "\n",
    "Where:\n",
    "- $m$ is the number of test examples.\n",
    "- $f(x^{(i)})$ is the predicted output for the $i$-th test example.\n",
    "- $y^{(i)}$ is the actual target value for the $i$-th test example.\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The training process in KNN is relatively simple because it does not involve explicit training or model fitting. Instead, the training set is stored, and predictions are made based on the proximity of data points at prediction time. The steps involved are:\n",
    "\n",
    "1. **Store the training set**: No actual model training occurs. All training data is stored and used at prediction time.\n",
    "2. **Determine K and the distance metric**: Select the number of nearest neighbors $K$ and the distance metric to be used (e.g., Euclidean or Manhattan distance).\n",
    "3. **For each test point**:\n",
    "   - Compute the distances from the test point to all points in the training set.\n",
    "   - Sort the distances and select the $K$ nearest neighbors.\n",
    "   - Calculate the average of the target values of the $K$ nearest neighbors to make the prediction.\n",
    "\n",
    "KNN is sensitive to the choice of $K$, as well as to the scale of the features. Feature scaling (e.g., normalization or standardization) is often necessary to prevent features with larger scales from dominating the distance calculation.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "The key hyperparameters of KNN Regressor are:\n",
    "- **K (number of neighbors)**: Controls how many nearest neighbors to consider when making a prediction. A small $K$ can lead to overfitting, while a large $K$ can lead to underfitting.\n",
    "- **Distance Metric**: The method used to compute distances between points. Common options include Euclidean, Manhattan, or Minkowski.\n",
    "- **Weights**: Determines how the neighbors contribute to the prediction. Options include:\n",
    "  - **Uniform**: All neighbors contribute equally.\n",
    "  - **Distance**: Neighbors closer to the query point have a higher weight.\n",
    "\n",
    "By selecting the right hyperparameters, KNN can provide accurate regression predictions. However, KNN is computationally expensive, especially with large datasets, because it requires calculating the distance from every point in the training set to the test point during prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true}_i} - y_{\\text{pred}_i})^2\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Squared Error (MSE)** is a widely used metric for evaluating the accuracy of regression models.\n",
    "- It measures the average squared difference between the predicted values ($y_{\\text{pred}}$) and the actual target values ($y_{\\text{true}}$).\n",
    "- The squared differences are averaged across all data points in the dataset.\n",
    "\n",
    "**Interpretation:**\n",
    "- A lower MSE indicates a better fit of the model to the data, as it means the model's predictions are closer to the actual values.\n",
    "- MSE is sensitive to outliers because the squared differences magnify the impact of large errors.\n",
    "- **Limitations:**\n",
    "  - MSE can be hard to interpret because it is in squared units of the target variable.\n",
    "  - It disproportionately penalizes larger errors due to the squaring process.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Root Mean Squared Error (RMSE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Root Mean Squared Error (RMSE)** is a variant of MSE that provides the square root of the average squared difference between predicted and actual values.\n",
    "- It is often preferred because it is in the same unit as the target variable, making it more interpretable.\n",
    "\n",
    "**Interpretation:**\n",
    "- Like MSE, a lower RMSE indicates a better fit of the model to the data.\n",
    "- RMSE is also sensitive to outliers due to the square root operation.\n",
    "- **Advantages over MSE:**\n",
    "  - RMSE provides a more intuitive interpretation since it is in the same scale as the target variable.\n",
    "  - It can be more directly compared to the values of the actual data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. R-squared ($R^2$)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **R-squared ($R^2$)**, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable ($y_{\\text{true}}$) that is predictable from the independent variable(s) ($y_{\\text{pred}}$) in a regression model.\n",
    "- It ranges from 0 to 1, where 0 indicates that the model does not explain any variance, and 1 indicates a perfect fit.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher $R^2$ value suggests that the model explains a larger proportion of the variance in the target variable.\n",
    "- However, $R^2$ does not provide information about the goodness of individual predictions or whether the model is overfitting or underfitting.\n",
    "- **Limitations:**\n",
    "  - $R^2$ can be misleading in cases of overfitting, especially with polynomial regression models. Even if $R^2$ is high, the model may not generalize well to unseen data.\n",
    "  - It doesn’t penalize for adding irrelevant predictors, so adjusted $R^2$ is often preferred for models with multiple predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Adjusted R-squared\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Adjusted } R^2 = 1 - \\left(1 - R^2\\right) \\frac{n-1}{n-p-1}\n",
    "$$\n",
    "where \\(n\\) is the number of data points and \\(p\\) is the number of predictors.\n",
    "\n",
    "**Description:**\n",
    "- **Adjusted R-squared** adjusts the R-squared value to account for the number of predictors in the model, helping to prevent overfitting when adding more terms to the model.\n",
    "- Unlike $R^2$, it can decrease if the additional predictors do not improve the model significantly.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher adjusted $R^2$ suggests that the model is not just overfitting, but has genuine explanatory power with the number of predictors taken into account.\n",
    "- It is especially useful when comparing models with different numbers of predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Mean Absolute Error (MAE)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true}_i} - y_{\\text{pred}_i}|\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- **Mean Absolute Error (MAE)** measures the average of the absolute errors between the predicted and actual values.\n",
    "- Unlike MSE and RMSE, MAE is not sensitive to outliers because it does not square the errors.\n",
    "\n",
    "**Interpretation:**\n",
    "- MAE provides a straightforward understanding of the average error magnitude.\n",
    "- A lower MAE suggests better model accuracy, but it may not highlight the impact of large errors as much as MSE or RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn template [scikit-learn: KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "\n",
    "### class sklearn.neighbors.KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "| **Parameter**        | **Description**                                                                                          | **Default**  |\n",
    "|---------------------|----------------------------------------------------------------------------------------------------------|--------------|\n",
    "| `n_neighbors`       | Number of neighbors to use for queries                                                                    | `5`          |\n",
    "| `weights`           | Weight function: 'uniform' (equal weights), 'distance' (inverse distance weights), or callable             | `'uniform'`  |\n",
    "| `algorithm`         | Algorithm for computing nearest neighbors: 'auto', 'ball_tree', 'kd_tree', or 'brute'                     | `'auto'`     |\n",
    "| `leaf_size`         | Leaf size for BallTree or KDTree. Affects construction speed and memory usage                            | `30`         |\n",
    "| `p`                 | Power parameter for Minkowski metric (p=1 Manhattan, p=2 Euclidean)                                       | `2`          |\n",
    "| `metric`            | Distance metric to use ('minkowski', 'manhattan', 'euclidean', etc.)                                      | `'minkowski'`|\n",
    "| `metric_params`     | Additional parameters for the metric function                                                             | `None`       |\n",
    "| `n_jobs`           | Number of parallel jobs for neighbor search. None=1, -1=all processors                                    | `None`       |\n",
    "\n",
    "---\n",
    "\n",
    "| **Attribute**           | **Description**                                                                                        |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------|\n",
    "| `effective_metric_`     | The distance metric used (may be synonym of specified metric)                                          |\n",
    "| `effective_metric_params_` | Additional parameters for the effective metric                                                        |\n",
    "| `n_features_in_`       | Number of features seen during fit                                                                     |\n",
    "| `feature_names_in_`    | Names of features seen during fit (if X has feature names)                                             |\n",
    "| `n_samples_fit_`       | Number of samples in the fitted data                                                                   |\n",
    "\n",
    "---\n",
    "\n",
    "| **Method**             | **Description**                                                                                        |\n",
    "|------------------------|--------------------------------------------------------------------------------------------------------|\n",
    "| `fit(X, y)`            | Fit the KNN regressor model                                                                           |\n",
    "| `predict(X)`           | Predict regression target for input samples                                                            |\n",
    "| `score(X, y)`          | Return R² score (coefficient of determination)                                                         |\n",
    "| `kneighbors(X)`        | Find K-neighbors of a point                                                                           |\n",
    "| `kneighbors_graph(X)`  | Compute the (weighted) graph of k-Neighbors                                                           |\n",
    "| `get_params()`         | Get parameters of the regressor                                                                        |\n",
    "| `set_params(**params)` | Set parameters of the regressor                                                                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXX regression - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
