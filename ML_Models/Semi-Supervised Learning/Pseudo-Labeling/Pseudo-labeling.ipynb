{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pseudo-Labeling Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Labeling\n",
    "\n",
    "---\n",
    "\n",
    "## Theory\n",
    "Pseudo-Labeling is a semi-supervised learning technique used to improve the performance of a model when labeled data is scarce. It leverages the model's own predictions to generate pseudo-labels for the unlabeled data. This approach combines labeled and unlabeled data to create a more robust model without the need for additional human labeling.\n",
    "\n",
    "The main idea is to:\n",
    "- Train a model on the labeled dataset.\n",
    "- Use the model to generate pseudo-labels for the unlabeled data.\n",
    "- Retrain the model using both the labeled data and the newly generated pseudo-labeled data.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundation\n",
    "- **Labeled Data**: \\( D_{\\text{labeled}} = \\{(x_i, y_i)\\} \\), where \\( x_i \\) represents the feature vector and \\( y_i \\) the true label.\n",
    "\n",
    "- **Unlabeled Data**: \\( D_{\\text{unlabeled}} = \\{x_i\\} \\), where the labels \\( y_i \\) are unknown.\n",
    "\n",
    "- **Pseudo-Labels**: For each unlabeled data point \\( x_i \\), the model generates a pseudo-label \\( \\hat{y}_i \\) based on its predicted probability:\n",
    "  $$ \\hat{y}_i = \\text{model}(x_i) $$\n",
    "\n",
    "- **Loss Function**:\n",
    "  The model is trained with a combined loss function for both labeled and pseudo-labeled data:\n",
    "  $$ L_{\\text{total}} = L_{\\text{labeled}} + \\lambda L_{\\text{pseudo}} $$ \n",
    "  Where:\n",
    "  - \\( L_{\\text{labeled}} \\) is the loss for the labeled data.\n",
    "  - \\( L_{\\text{pseudo}} \\) is the loss for the pseudo-labeled data.\n",
    "  - \\( \\lambda \\) is a weighting factor that controls the influence of pseudo-labels.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm Steps\n",
    "1. **Train on Labeled Data**:\n",
    "   - Train the model using the labeled dataset \\( D_{\\text{labeled}} \\).\n",
    "\n",
    "2. **Generate Pseudo-Labels**:\n",
    "   - Use the trained model to predict labels for the unlabeled data \\( D_{\\text{unlabeled}} \\).\n",
    "\n",
    "3. **Filter Pseudo-Labels**:\n",
    "   - Optionally, select only confident predictions (e.g., predictions above a certain threshold).\n",
    "\n",
    "4. **Retrain the Model**:\n",
    "   - Combine the labeled data and pseudo-labeled data and retrain the model using the combined dataset.\n",
    "\n",
    "5. **Evaluate the Model**:\n",
    "   - Evaluate the model's performance on a validation set or test set.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters\n",
    "- **Threshold**: Confidence threshold for accepting pseudo-labels.\n",
    "- **Weighting Factor \\( \\lambda \\)**: Balances the contribution of labeled and pseudo-labeled data.\n",
    "- **Unlabeled Data**: Amount of unlabeled data available for pseudo-labeling.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "- **Reduces Labeling Cost**: Leverages unlabeled data, which is often abundant.\n",
    "- **Improves Model Performance**: Utilizes the full potential of available data.\n",
    "- **Scalable**: Effective for large datasets where labeling is expensive or time-consuming.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages\n",
    "- **Model Bias**: Incorrect pseudo-labels can introduce noise and harm model performance.\n",
    "- **Confidence Threshold Tuning**: Choosing the right threshold for pseudo-label selection is crucial.\n",
    "- **Risk of Overfitting**: Over-relying on pseudo-labeled data may lead to overfitting on noisy labels.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Tips\n",
    "- **Threshold Selection**: Use cross-validation or grid search to find the optimal confidence threshold.\n",
    "- **Iterative Refinement**: Apply pseudo-labeling iteratively, refining the model over time.\n",
    "- **Regularization**: Use regularization techniques to prevent overfitting, especially when using noisy pseudo-labels.\n",
    "- **Use a Robust Base Model**: A well-trained model is crucial for generating high-quality pseudo-labels.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "- **Image Classification**: Labeling large amounts of unlabeled image data.\n",
    "- **Natural Language Processing**: Generating pseudo-labels for text classification tasks.\n",
    "- **Medical Imaging**: Annotating medical images where labeled data is limited.\n",
    "- **Speech Recognition**: Using unlabeled audio data to improve speech recognition models.\n",
    "\n",
    "Pseudo-labeling is a simple but effective technique to leverage unlabeled data and improve model performance when labeled data is scarce. It is especially useful in real-world scenarios where obtaining labeled data can be expensive or time-consuming.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Evaluation for Pseudo-Labeling**\n",
    "\n",
    "### 1. Accuracy\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- Measures the overall correctness of the pseudo-labeling process.\n",
    "- Compares the number of correctly pseudo-labeled samples to the total pseudo-labeled samples.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher accuracy indicates better pseudo-labeling quality.\n",
    "- Can be misleading if the dataset is imbalanced.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Precision\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- Measures the proportion of correctly pseudo-labeled positive samples out of all predicted positives.\n",
    "- Helps assess the reliability of pseudo-labels generated by the model.\n",
    "\n",
    "**Interpretation:**\n",
    "- High precision means fewer false positives in the pseudo-labeling process.\n",
    "- Important when incorrect pseudo-labels can harm the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Recall (Sensitivity)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- Measures how many actual positive samples were correctly pseudo-labeled.\n",
    "- Important when missing pseudo-labels for positives is costly.\n",
    "\n",
    "**Interpretation:**\n",
    "- High recall indicates fewer missed positive samples.\n",
    "- Crucial for ensuring the model captures as many relevant pseudo-labels as possible.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. F1-Score\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- A balance between precision and recall in pseudo-labeling.\n",
    "- Useful when evaluating the trade-off between the two.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher F1-score indicates a balanced performance.\n",
    "- Important when both false positives and false negatives are problematic.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Confusion Matrix\n",
    "\n",
    "**Description:**\n",
    "- A table summarizing true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) for pseudo-labeled data.\n",
    "\n",
    "**Interpretation:**\n",
    "- Helps visualize errors in pseudo-labeling.\n",
    "- Important to analyze both pseudo-labeled and true labeled samples.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. AUC-ROC Curve\n",
    "\n",
    "**Description:**\n",
    "- Plots True Positive Rate (TPR) vs. False Positive Rate (FPR) for pseudo-labeled data.\n",
    "- AUC (Area Under Curve) measures the performance of the pseudo-labeling process.\n",
    "\n",
    "**Interpretation:**\n",
    "- **AUC = 1** → Perfect pseudo-labeling.\n",
    "- **AUC > 0.8** → Strong pseudo-labeling.\n",
    "- **AUC = 0.5** → Random pseudo-labeling.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Labeling Accuracy (Pseudo-Label Quality)\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Labeling Accuracy} = \\frac{\\text{Correctly Pseudo-Labeled Samples}}{\\text{Total Pseudo-Labeled Samples}}\n",
    "$$\n",
    "\n",
    "**Description:**\n",
    "- Measures how accurately the model assigns pseudo-labels to unlabeled data.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher accuracy means better pseudo-labeling quality.\n",
    "- Poor labeling can degrade the overall model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Number of Iterations Until Convergence\n",
    "\n",
    "**Description:**\n",
    "- Pseudo-labeling often involves multiple iterations to refine labels.\n",
    "- The number of iterations until performance stabilizes is an important metric.\n",
    "\n",
    "**Interpretation:**\n",
    "- Too many iterations may indicate overfitting.\n",
    "- Faster convergence is desirable for efficient model training.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. k-Fold Cross Validation\n",
    "\n",
    "**Description:**\n",
    "- Splits the dataset into \\( k \\) subsets to evaluate the generalization of pseudo-labels.\n",
    "- Helps assess the stability of the model after pseudo-labeling.\n",
    "\n",
    "**Interpretation:**\n",
    "- Reduces overfitting risk.\n",
    "- Provides a more reliable estimate of model performance after pseudo-labeling.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Labeling (General Overview)\n",
    "\n",
    "### Pseudo-Labeling Process (Implemented in PyTorch)\n",
    "\n",
    "| **Parameter**   | **Description**                                                                 |\n",
    "|-----------------|-------------------------------------------------------------------------------|\n",
    "| base_model      | The neural network model used for predictions and pseudo-label generation.    |\n",
    "| threshold       | Minimum confidence required to assign pseudo-labels to unlabeled samples.     |\n",
    "| max_iter        | Number of iterations for the pseudo-labeling process.                         |\n",
    "| unlabeled_data  | The dataset that contains the unlabeled samples.                             |\n",
    "| labeled_data    | The dataset that contains the labeled samples.                               |\n",
    "| batch_size      | Number of samples per iteration for training.                                |\n",
    "| learning_rate   | The rate at which the model's weights are updated during training.           |\n",
    "\n",
    "-\n",
    "\n",
    "| **Attribute**         | **Description**                                                                 |\n",
    "|-----------------------|-------------------------------------------------------------------------------|\n",
    "| pseudo_labels         | The pseudo-labels assigned to the unlabeled samples based on the model's predictions. |\n",
    "| confidence_threshold  | The threshold probability above which predictions are considered confident. |\n",
    "| iteration             | The current iteration number in the pseudo-labeling process.                 |\n",
    "\n",
    "-\n",
    "\n",
    "| **Method**            | **Description**                                                                 |\n",
    "|-----------------------|-------------------------------------------------------------------------------|\n",
    "| fit(X, y)             | Train the model on the labeled data and pseudo-labeled data in an iterative process. |\n",
    "| predict(X)            | Predict labels for input data `X` using the trained model.                   |\n",
    "| predict_proba(X)      | Predict class probabilities for input data `X`.                              |\n",
    "\n",
    "-\n",
    "\n",
    "[Documentation](https://pytorch.org/tutorials/beginner/supervised_learning/semisupervised_learning_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Labeling - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on pseudo-labeled data: 0.1019\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Data Loading and Processing\n",
    "# Load the Digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Split into labeled and unlabeled sets (95% unlabeled)\n",
    "X_train, X_unlabeled, y_train, y_unlabeled = train_test_split(X, y, test_size=0.95, random_state=42)\n",
    "\n",
    "# Set some of the labels to -1 for unlabeled\n",
    "y_train[-100:] = -1  # 100 samples as unlabeled\n",
    "\n",
    "# Convert to torch tensors\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "y_train_tensor = torch.Tensor(y_train).long()\n",
    "X_unlabeled_tensor = torch.Tensor(X_unlabeled)\n",
    "\n",
    "# Create DataLoader\n",
    "labeled_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "labeled_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 2. Model Definition\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 64)\n",
    "        self.fc2 = nn.Linear(64, 10)  # 10 output classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 3. Initial Training on Labeled Data\n",
    "def train(model, labeled_loader):\n",
    "    model.train()\n",
    "    for inputs, labels in labeled_loader:\n",
    "        # Remove samples where label is -1\n",
    "        valid_indices = labels != -1\n",
    "        inputs = inputs[valid_indices]\n",
    "        labels = labels[valid_indices]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Train on labeled data\n",
    "train(model, labeled_loader)\n",
    "\n",
    "# 4. Generate Pseudo-Labels for Unlabeled Data\n",
    "def generate_pseudo_labels(model, X_unlabeled_tensor, threshold=0.75):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_unlabeled_tensor)\n",
    "        probabilities = torch.softmax(outputs, dim=1)\n",
    "        pseudo_labels = torch.max(probabilities, dim=1)[1]\n",
    "        confidence = torch.max(probabilities, dim=1)[0]\n",
    "        \n",
    "    # Apply threshold for confidence\n",
    "    high_confidence_mask = confidence > threshold\n",
    "    pseudo_labels[~high_confidence_mask] = -1  # Set uncertain predictions back to -1\n",
    "    return pseudo_labels\n",
    "\n",
    "# Get pseudo-labels for unlabeled data\n",
    "pseudo_labels = generate_pseudo_labels(model, X_unlabeled_tensor)\n",
    "\n",
    "# 5. Combine Labeled and Pseudo-Labeled Data\n",
    "X_combined = torch.cat((X_train_tensor, X_unlabeled_tensor), dim=0)\n",
    "y_combined = torch.cat((y_train_tensor, pseudo_labels), dim=0)\n",
    "\n",
    "# Filter out the -1 values (pseudo-labels) for training\n",
    "combined_dataset = TensorDataset(X_combined, y_combined)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 6. Retrain Model Using Both Labeled and Pseudo-Labeled Data\n",
    "train(model, combined_loader)\n",
    "\n",
    "# 7. Model Evaluation\n",
    "# In this example, we're using a subset of unlabeled data for evaluation purposes\n",
    "y_pred = model(X_unlabeled_tensor)\n",
    "_, predicted_labels = torch.max(y_pred, 1)\n",
    "\n",
    "# Evaluate accuracy (since we only have labeled data for evaluation)\n",
    "accuracy = accuracy_score(y_unlabeled, predicted_labels.numpy())\n",
    "print(f\"Accuracy on pseudo-labeled data: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
