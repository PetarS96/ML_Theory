{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Principal Componenet analysis Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "## Theory\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving as much variance as possible. It achieves this by identifying the directions (principal components) in which the data varies the most. PCA is widely used in data visualization, noise reduction, and feature extraction.\n",
    "\n",
    "The main idea is to:\n",
    "- Compute the covariance matrix of the data.\n",
    "- Perform eigenvalue decomposition to identify the principal components.\n",
    "- Project the data onto the principal components to reduce dimensionality.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundation\n",
    "- **Covariance Matrix**:\n",
    "  The covariance matrix \\( \\Sigma \\) of the data \\( X \\) is computed as:\n",
    "  $$ \\Sigma = \\frac{1}{n-1} X^T X $$\n",
    "  - \\( X \\): Data matrix (centered around the mean).\n",
    "  - \\( n \\): Number of data points.\n",
    "\n",
    "- **Eigenvalue Decomposition**:\n",
    "  The covariance matrix is decomposed into eigenvalues \\( \\lambda_i \\) and eigenvectors \\( v_i \\):\n",
    "  $$ \\Sigma v_i = \\lambda_i v_i $$\n",
    "  - \\( \\lambda_i \\): Eigenvalues (represent the amount of variance explained by each principal component).\n",
    "  - \\( v_i \\): Eigenvectors (represent the directions of the principal components).\n",
    "\n",
    "- **Principal Components**:\n",
    "  The principal components are the eigenvectors sorted by their corresponding eigenvalues in descending order.\n",
    "\n",
    "- **Dimensionality Reduction**:\n",
    "  The data is projected onto the top \\( k \\) principal components to obtain the reduced-dimensional representation:\n",
    "  $$ Y = X V_k $$\n",
    "  - \\( Y \\): Reduced-dimensional data.\n",
    "  - \\( V_k \\): Matrix of the top \\( k \\) eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm Steps\n",
    "1. **Standardization**:\n",
    "   - Standardize the data to have zero mean and unit variance.\n",
    "\n",
    "2. **Covariance Matrix**:\n",
    "   - Compute the covariance matrix of the standardized data.\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "   - Perform eigenvalue decomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Sorting**:\n",
    "   - Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "\n",
    "5. **Projection**:\n",
    "   - Project the data onto the top \\( k \\) eigenvectors to obtain the reduced-dimensional representation.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters\n",
    "- **n_components**: The number of principal components to retain.\n",
    "- **svd_solver**: The solver to use for eigenvalue decomposition (e.g., `auto`, `full`, `arpack`).\n",
    "- **whiten**: Whether to whiten the data (scale the components to unit variance).\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "- Reduces dimensionality while preserving variance.\n",
    "- Improves computational efficiency by reducing the number of features.\n",
    "- Helps in visualizing high-dimensional data.\n",
    "- Removes multicollinearity between features.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages\n",
    "- Assumes linear relationships between features.\n",
    "- Sensitive to the scaling of features.\n",
    "- May not capture complex structures in the data.\n",
    "- Interpretability of principal components can be challenging.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Tips\n",
    "- **Standardize** the data before applying PCA to ensure equal contribution from all features.\n",
    "- Use **scree plots** to determine the optimal number of principal components.\n",
    "- Consider **Kernel PCA** for non-linear dimensionality reduction.\n",
    "- Use **explained variance ratio** to understand the contribution of each principal component.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "- Data visualization (e.g., 2D/3D plots of high-dimensional data)\n",
    "- Noise reduction\n",
    "- Feature extraction for machine learning models\n",
    "- Image compression\n",
    "- Genomics and bioinformatics\n",
    "\n",
    "PCA is a powerful and widely-used technique for dimensionality reduction. While it has limitations, it is a valuable tool for many real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation for Principal Component Analysis (PCA)\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Explained Variance Ratio\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Explained Variance Ratio} = \\frac{\\text{Variance of Principal Component}}{\\text{Total Variance}}\n",
    "$$\n",
    "**Description:**\n",
    "- Measures the proportion of the dataset's variance explained by each principal component.\n",
    "- Cumulative explained variance indicates how much information is retained by the top \\( k \\) components.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher values indicate that the component captures more variance.\n",
    "- Useful for determining the optimal number of components to retain.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Cumulative Explained Variance\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Cumulative Explained Variance} = \\sum_{i=1}^k \\text{Explained Variance Ratio}_i\n",
    "$$\n",
    "**Description:**\n",
    "- Measures the total proportion of variance explained by the first \\( k \\) principal components.\n",
    "\n",
    "**Interpretation:**\n",
    "- Values close to 1 indicate that the top \\( k \\) components capture most of the variance.\n",
    "- Helps decide how many components to keep for dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Scree Plot Analysis\n",
    "**Description:**\n",
    "- A graphical representation of the explained variance ratio for each principal component.\n",
    "- Helps identify the \"elbow point\" where adding more components provides diminishing returns.\n",
    "\n",
    "**Interpretation:**\n",
    "- The elbow point suggests the optimal number of components to retain.\n",
    "- Components after the elbow contribute little to explaining variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Reconstruction Error\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Reconstruction Error} = \\frac{1}{N} \\sum_{i=1}^N ||x_i - \\hat{x}_i||^2\n",
    "$$\n",
    "**Description:**\n",
    "- Measures the error between the original data and the data reconstructed from the principal components.\n",
    "- Indicates how well the PCA preserves the original data structure.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower values indicate better reconstruction.\n",
    "- Useful for assessing the quality of dimensionality reduction.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Singular Values\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Singular Values} = \\sqrt{\\text{Eigenvalues of Covariance Matrix}}\n",
    "$$\n",
    "**Description:**\n",
    "- Represents the importance of each principal component.\n",
    "- Larger singular values correspond to components that capture more variance.\n",
    "\n",
    "**Interpretation:**\n",
    "- Useful for understanding the relative importance of each component.\n",
    "- Helps identify components that can be discarded.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Principal Component Loadings\n",
    "**Description:**\n",
    "- Represents the contribution of each original feature to a principal component.\n",
    "- Loadings indicate the direction and magnitude of the feature's influence.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher absolute values indicate stronger influence.\n",
    "- Useful for interpreting the meaning of principal components.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Correlation Circle (Biplot)\n",
    "**Description:**\n",
    "- A graphical representation of the relationships between original features and principal components.\n",
    "- Helps visualize how features contribute to the components.\n",
    "\n",
    "**Interpretation:**\n",
    "- Features closer to the circle's edge have stronger influence.\n",
    "- Useful for feature selection and interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Dimensionality Reduction Effectiveness\n",
    "**Description:**\n",
    "- Evaluates the effectiveness of PCA in reducing the number of dimensions while retaining information.\n",
    "- Measured by comparing performance metrics (e.g., classification accuracy) before and after PCA.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher retained performance indicates effective dimensionality reduction.\n",
    "- Useful for assessing the trade-off between complexity and information loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Computational Efficiency\n",
    "**Description:**\n",
    "- Evaluates the computational cost of performing PCA, including time and memory usage.\n",
    "- Important for large datasets or real-time applications.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower computational cost indicates better scalability.\n",
    "- Useful for assessing the practicality of PCA for specific use cases.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Outlier Detection\n",
    "**Description:**\n",
    "- Evaluates the ability of PCA to detect outliers by examining the reconstruction error or the distance of data points from the principal component subspace.\n",
    "\n",
    "**Interpretation:**\n",
    "- Points with high reconstruction error or large distances are potential outliers.\n",
    "- Useful for identifying anomalies in the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn template [sckit-kit: model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Componenet analysis - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
