{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autoencoders Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "---\n",
    "\n",
    "## Theory\n",
    "Autoencoders are a type of neural network used for unsupervised learning. They are designed to learn efficient representations (encodings) of input data, typically for the purpose of dimensionality reduction or feature learning. An autoencoder consists of two main parts: an encoder that maps the input data into a lower-dimensional latent space, and a decoder that reconstructs the input data from the latent representation.\n",
    "\n",
    "The main idea is to:\n",
    "- Encode the input data into a compact latent representation.\n",
    "- Decode the latent representation back to the original input space.\n",
    "- Minimize the reconstruction error between the input and the output.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundation\n",
    "- **Encoder**:\n",
    "  The encoder maps the input \\( x \\) to a latent representation \\( z \\):\n",
    "  $$ z = f(x; \\theta_e) $$\n",
    "  - \\( f \\): Encoding function (e.g., a neural network).\n",
    "  - \\( \\theta_e \\): Parameters of the encoder.\n",
    "\n",
    "- **Decoder**:\n",
    "  The decoder maps the latent representation \\( z \\) back to the reconstructed input \\( \\hat{x} \\):\n",
    "  $$ \\hat{x} = g(z; \\theta_d) $$\n",
    "  - \\( g \\): Decoding function (e.g., a neural network).\n",
    "  - \\( \\theta_d \\): Parameters of the decoder.\n",
    "\n",
    "- **Reconstruction Loss**:\n",
    "  The objective is to minimize the reconstruction loss, typically measured using mean squared error (MSE) or cross-entropy:\n",
    "  $$ \\mathcal{L} = \\mathbb{E}_{x \\sim p_{\\text{data}}(x)} \\left[ \\ell(x, \\hat{x}) \\right] $$\n",
    "  - \\( \\ell \\): Loss function (e.g., MSE, cross-entropy).\n",
    "  - \\( p_{\\text{data}}(x) \\): Data distribution.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm Steps\n",
    "1. **Define the Encoder and Decoder**:\n",
    "   - Design the architecture of the encoder and decoder networks.\n",
    "   - Choose appropriate activation functions and layer sizes.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - Pass the input data through the encoder to obtain the latent representation.\n",
    "   - Pass the latent representation through the decoder to obtain the reconstructed output.\n",
    "\n",
    "3. **Compute the Reconstruction Loss**:\n",
    "   - Calculate the reconstruction loss between the input and the reconstructed output.\n",
    "\n",
    "4. **Backward Pass**:\n",
    "   - Compute the gradients of the loss with respect to the network parameters.\n",
    "   - Update the parameters using an optimization algorithm (e.g., gradient descent, Adam).\n",
    "\n",
    "5. **Repeat**:\n",
    "   - Repeat the forward and backward passes for multiple epochs until the loss converges.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters\n",
    "- **hidden_layer_sizes**: The number of neurons in each hidden layer of the encoder and decoder.\n",
    "- **activation**: The activation function used in the hidden layers (e.g., ReLU, sigmoid, tanh).\n",
    "- **optimizer**: The optimization algorithm used to update the network parameters (e.g., SGD, Adam).\n",
    "- **learning_rate**: The step size for the optimization algorithm.\n",
    "- **batch_size**: The number of samples per batch for mini-batch gradient descent.\n",
    "- **epochs**: The number of times the entire dataset is passed through the network during training.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "- Can learn complex, non-linear mappings between input and latent spaces.\n",
    "- Useful for unsupervised learning and feature extraction.\n",
    "- Can be used for data denoising and anomaly detection.\n",
    "- Can generate new data samples by sampling from the latent space.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages\n",
    "- Can be computationally expensive, especially for large datasets and deep networks.\n",
    "- May overfit to the training data if not properly regularized.\n",
    "- The latent space may not be easily interpretable.\n",
    "- May require careful tuning of hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Tips\n",
    "- **Regularization**: Use techniques like dropout, L1/L2 regularization, or early stopping to prevent overfitting.\n",
    "- **Hyperparameter Tuning**: Experiment with different architectures, activation functions, and learning rates to find the best configuration.\n",
    "- **Data Preprocessing**: Standardize or normalize the input data to improve training stability.\n",
    "- **Use Pre-trained Models**: Consider using pre-trained autoencoders as a starting point for transfer learning.\n",
    "- **Visualize the Latent Space**: Use techniques like t-SNE or PCA to visualize the latent space and understand the learned representations.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "- Dimensionality reduction\n",
    "- Feature learning and extraction\n",
    "- Data denoising\n",
    "- Anomaly detection\n",
    "- Generative modeling (e.g., variational autoencoders, generative adversarial networks)\n",
    "- Image and text data processing\n",
    "\n",
    "Autoencoders are a versatile tool for unsupervised learning and can be applied to a wide range of tasks. While they have some limitations, they are a powerful technique for learning efficient representations of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation for Autoencoders\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Reconstruction Error\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Reconstruction Error} = \\frac{1}{N} \\sum_{i=1}^N ||x_i - \\hat{x}_i||^2\n",
    "$$\n",
    "**Description:**\n",
    "- Measures the difference between the original input data and the reconstructed data.\n",
    "- Commonly used loss function in autoencoders.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower values indicate better reconstruction.\n",
    "- Useful for assessing the quality of the encoder and decoder.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Mean Squared Error (MSE)\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\hat{x}_i)^2\n",
    "$$\n",
    "**Description:**\n",
    "- Measures the average squared difference between the original and reconstructed data.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower values indicate better reconstruction.\n",
    "- Commonly used for continuous data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Mean Absolute Error (MAE)\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^N |x_i - \\hat{x}_i|\n",
    "$$\n",
    "**Description:**\n",
    "- Measures the average absolute difference between the original and reconstructed data.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower values indicate better reconstruction.\n",
    "- Less sensitive to outliers compared to MSE.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Structural Similarity Index (SSIM)\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{SSIM}(x, \\hat{x}) = \\frac{(2\\mu_x\\mu_{\\hat{x}} + c_1)(2\\sigma_{x\\hat{x}} + c_2)}{(\\mu_x^2 + \\mu_{\\hat{x}}^2 + c_1)(\\sigma_x^2 + \\sigma_{\\hat{x}}^2 + c_2)}\n",
    "$$\n",
    "**Description:**\n",
    "- Measures the structural similarity between the original and reconstructed images.\n",
    "- Useful for image data.\n",
    "\n",
    "**Interpretation:**\n",
    "- Values range from -1 to 1.\n",
    "- Higher values indicate better similarity.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Feature Visualization\n",
    "**Description:**\n",
    "- Visualizes the learned features in the latent space.\n",
    "- Helps understand what the autoencoder has learned.\n",
    "\n",
    "**Interpretation:**\n",
    "- Clear and meaningful features indicate good learning.\n",
    "- Useful for interpretability and debugging.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Latent Space Analysis\n",
    "**Description:**\n",
    "- Evaluates the structure and properties of the latent space.\n",
    "- Can include visualizations like t-SNE or PCA of the latent space.\n",
    "\n",
    "**Interpretation:**\n",
    "- Well-structured latent space indicates effective dimensionality reduction.\n",
    "- Useful for understanding the autoencoder's internal representation.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Reconstruction Quality on Test Data\n",
    "**Description:**\n",
    "- Evaluates the autoencoder's ability to generalize to unseen data.\n",
    "- Measures reconstruction error on a test set.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower reconstruction error on test data indicates better generalization.\n",
    "- Useful for assessing overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Dimensionality Reduction Effectiveness\n",
    "**Description:**\n",
    "- Evaluates the effectiveness of the autoencoder in reducing the number of dimensions while retaining information.\n",
    "- Measured by comparing performance metrics (e.g., classification accuracy) before and after dimensionality reduction.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher retained performance indicates effective dimensionality reduction.\n",
    "- Useful for assessing the trade-off between complexity and information loss.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Computational Efficiency\n",
    "**Description:**\n",
    "- Evaluates the computational cost of training and using the autoencoder, including time and memory usage.\n",
    "- Important for large datasets or real-time applications.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower computational cost indicates better scalability.\n",
    "- Useful for assessing the practicality of the autoencoder for specific use cases.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Robustness to Noise\n",
    "**Description:**\n",
    "- Evaluates the autoencoder's ability to handle noisy input data.\n",
    "- Measures reconstruction error on noisy data.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower reconstruction error on noisy data indicates better robustness.\n",
    "- Useful for assessing the autoencoder's ability to denoise.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. Anomaly Detection\n",
    "**Description:**\n",
    "- Evaluates the autoencoder's ability to detect outliers by examining the reconstruction error.\n",
    "- Points with high reconstruction error are potential anomalies.\n",
    "\n",
    "**Interpretation:**\n",
    "- Points with high reconstruction error are likely outliers.\n",
    "- Useful for identifying anomalies in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. Overfitting and Underfitting\n",
    "**Description:**\n",
    "- Evaluates the autoencoder's performance on training and validation sets.\n",
    "- Overfitting occurs when the model performs well on the training set but poorly on the validation set.\n",
    "- Underfitting occurs when the model performs poorly on both sets.\n",
    "\n",
    "**Interpretation:**\n",
    "- Balanced performance on both sets indicates a well-trained model.\n",
    "- Useful for tuning hyperparameters and model architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Learning Curves\n",
    "**Description:**\n",
    "- Plots the training and validation loss over epochs.\n",
    "- Helps identify overfitting, underfitting, and optimal training duration.\n",
    "\n",
    "**Interpretation:**\n",
    "- Convergence of training and validation loss indicates a well-trained model.\n",
    "- Useful for monitoring training progress and adjusting hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. Latent Space Interpolation\n",
    "**Description:**\n",
    "- Evaluates the smoothness and continuity of the latent space by interpolating between points.\n",
    "- Helps understand the structure of the latent space.\n",
    "\n",
    "**Interpretation:**\n",
    "- Smooth and meaningful interpolations indicate a well-structured latent space.\n",
    "- Useful for generating new data and understanding the autoencoder's learned representations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "\n",
    "### class Autoencoder(input_dim, encoding_dim, *, hidden_layers=None, activation='relu', optimizer='adam', loss='mean_squared_error', epochs=100, batch_size=32, validation_split=0.1, random_state=None)\n",
    "\n",
    "| **Parameter**               | **Description**                                                                                                                                     | **Default**      |\n",
    "|----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|------------------|\n",
    "| input_dim                 | Dimension of the input data                                                                                                                         | -                |\n",
    "| encoding_dim              | Dimension of the encoded representation                                                                                                             | -                |\n",
    "| hidden_layers             | List of integers specifying the number of units in each hidden layer (optional)                                                                      | None             |\n",
    "| activation                | Activation function to use for the hidden layers ('relu', 'sigmoid', 'tanh', etc.)                                                                  | 'relu'           |\n",
    "| optimizer                 | Optimizer to use for training ('adam', 'sgd', 'rmsprop', etc.)                                                                                       | 'adam'           |\n",
    "| loss                      | Loss function to use for training ('mean_squared_error', 'binary_crossentropy', etc.)                                                                | 'mean_squared_error' |\n",
    "| epochs                    | Number of epochs to train the model                                                                                                                 | 100              |\n",
    "| batch_size                | Number of samples per gradient update                                                                                                               | 32               |\n",
    "| validation_split          | Fraction of the training data to be used as validation data                                                                                          | 0.1              |\n",
    "| random_state              | Seed for the random number generator for reproducibility                                                                                            | None             |\n",
    "\n",
    "-\n",
    "\n",
    "| **Attribute**              | **Description**                                                                                                                                     |\n",
    "|----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| encoder_                 | The encoder part of the autoencoder, which maps input data to the encoded representation                                                             |\n",
    "| decoder_                 | The decoder part of the autoencoder, which maps the encoded representation back to the input data                                                    |\n",
    "| history_                 | Training history, including loss and validation loss over epochs                                                                                     |\n",
    "| input_dim_               | Dimension of the input data as used in the model                                                                                                     |\n",
    "| encoding_dim_            | Dimension of the encoded representation as used in the model                                                                                          |\n",
    "\n",
    "-\n",
    "\n",
    "| **Method**                 | **Description**                                                                                                                                     |\n",
    "|----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| fit(X[, y])             | Train the autoencoder using the input data X                                                                                                         |\n",
    "| transform(X)            | Encode the input data X to the encoded representation                                                                                                 |\n",
    "| inverse_transform(X)    | Decode the encoded data X back to the original input space                                                                                           |\n",
    "| predict(X)              | Reconstruct the input data X using the autoencoder                                                                                                   |\n",
    "| evaluate(X[, y])        | Evaluate the autoencoder on the input data X, returning the loss value                                                                               |\n",
    "\n",
    "[Documentation](https://keras.io/api/layers/autoencoder_layers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders - Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
      "0     0.0     0.0     5.0    13.0     9.0     1.0     0.0     0.0     0.0   \n",
      "1     0.0     0.0     0.0    12.0    13.0     5.0     0.0     0.0     0.0   \n",
      "2     0.0     0.0     0.0     4.0    15.0    12.0     0.0     0.0     0.0   \n",
      "3     0.0     0.0     7.0    15.0    13.0     1.0     0.0     0.0     0.0   \n",
      "4     0.0     0.0     0.0     1.0    11.0     0.0     0.0     0.0     0.0   \n",
      "\n",
      "   pixel9  ...  pixel55  pixel56  pixel57  pixel58  pixel59  pixel60  pixel61  \\\n",
      "0     0.0  ...      0.0      0.0      0.0      6.0     13.0     10.0      0.0   \n",
      "1     0.0  ...      0.0      0.0      0.0      0.0     11.0     16.0     10.0   \n",
      "2     0.0  ...      0.0      0.0      0.0      0.0      3.0     11.0     16.0   \n",
      "3     8.0  ...      0.0      0.0      0.0      7.0     13.0     13.0      9.0   \n",
      "4     0.0  ...      0.0      0.0      0.0      0.0      2.0     16.0      4.0   \n",
      "\n",
      "   pixel62  pixel63  target  \n",
      "0      0.0      0.0       0  \n",
      "1      0.0      0.0       1  \n",
      "2      9.0      0.0       2  \n",
      "3      0.0      0.0       3  \n",
      "4      0.0      0.0       4  \n",
      "\n",
      "[5 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd  # Added pandas import\n",
    "import setuptools as _distutils\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "# Load the Digits dataset from sklearn.datasets\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Data Processing\n",
    "# Convert the data to a DataFrame for better visualization\n",
    "df = pd.DataFrame(data.data, columns=[f'pixel{i}' for i in range(64)])\n",
    "df['target'] = data.target\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for missing values:\n",
      "pixel0     0\n",
      "pixel1     0\n",
      "pixel2     0\n",
      "pixel3     0\n",
      "pixel4     0\n",
      "          ..\n",
      "pixel60    0\n",
      "pixel61    0\n",
      "pixel62    0\n",
      "pixel63    0\n",
      "target     0\n",
      "Length: 65, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nChecking for missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 21:24:11.617610: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - loss: 1.1843 - val_loss: 1.1034\n",
      "Epoch 2/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.1778 - val_loss: 1.0780\n",
      "Epoch 3/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.1668 - val_loss: 1.0535\n",
      "Epoch 4/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.2106 - val_loss: 1.0297\n",
      "Epoch 5/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.1773 - val_loss: 1.0063\n",
      "Epoch 6/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.0678 - val_loss: 0.9834\n",
      "Epoch 7/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.0896 - val_loss: 0.9609\n",
      "Epoch 8/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1.0435 - val_loss: 0.9387\n",
      "Epoch 9/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.0651 - val_loss: 0.9171\n",
      "Epoch 10/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.0136 - val_loss: 0.8962\n",
      "Epoch 11/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.0226 - val_loss: 0.8761\n",
      "Epoch 12/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.9774 - val_loss: 0.8570\n",
      "Epoch 13/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.9721 - val_loss: 0.8390\n",
      "Epoch 14/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.9961 - val_loss: 0.8221\n",
      "Epoch 15/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.9504 - val_loss: 0.8064\n",
      "Epoch 16/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8657 - val_loss: 0.7920\n",
      "Epoch 17/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.9800 - val_loss: 0.7787\n",
      "Epoch 18/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.8743 - val_loss: 0.7667\n",
      "Epoch 19/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.8651 - val_loss: 0.7557\n",
      "Epoch 20/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.9227 - val_loss: 0.7457\n",
      "Epoch 21/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.8466 - val_loss: 0.7365\n",
      "Epoch 22/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8636 - val_loss: 0.7282\n",
      "Epoch 23/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7881 - val_loss: 0.7206\n",
      "Epoch 24/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.8226 - val_loss: 0.7136\n",
      "Epoch 25/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8269 - val_loss: 0.7072\n",
      "Epoch 26/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.7750 - val_loss: 0.7012\n",
      "Epoch 27/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.7958 - val_loss: 0.6957\n",
      "Epoch 28/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.8461 - val_loss: 0.6904\n",
      "Epoch 29/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.8200 - val_loss: 0.6856\n",
      "Epoch 30/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.8096 - val_loss: 0.6810\n",
      "Epoch 31/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.8529 - val_loss: 0.6766\n",
      "Epoch 32/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.7597 - val_loss: 0.6725\n",
      "Epoch 33/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.8114 - val_loss: 0.6687\n",
      "Epoch 34/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7653 - val_loss: 0.6651\n",
      "Epoch 35/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.7754 - val_loss: 0.6617\n",
      "Epoch 36/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.7639 - val_loss: 0.6585\n",
      "Epoch 37/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7665 - val_loss: 0.6554\n",
      "Epoch 38/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.7507 - val_loss: 0.6525\n",
      "Epoch 39/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.8323 - val_loss: 0.6497\n",
      "Epoch 40/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7355 - val_loss: 0.6470\n",
      "Epoch 41/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.7639 - val_loss: 0.6445\n",
      "Epoch 42/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7353 - val_loss: 0.6421\n",
      "Epoch 43/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7183 - val_loss: 0.6397\n",
      "Epoch 44/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.7305 - val_loss: 0.6374\n",
      "Epoch 45/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7594 - val_loss: 0.6353\n",
      "Epoch 46/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.7389 - val_loss: 0.6331\n",
      "Epoch 47/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.7102 - val_loss: 0.6311\n",
      "Epoch 48/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7581 - val_loss: 0.6292\n",
      "Epoch 49/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.7582 - val_loss: 0.6273\n",
      "Epoch 50/50\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 0.7715 - val_loss: 0.6254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x731226277ad0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Definition\n",
    "# Define the autoencoder model\n",
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 32  # Number of dimensions for the encoded representation\n",
    "\n",
    "# Encoder\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "\n",
    "# Encoder model\n",
    "encoder = Model(input_layer, encoded)\n",
    "\n",
    "# Decoder model\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# Compile the autoencoder\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Model Training\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=50, batch_size=256, shuffle=True, validation_data=(X_test, X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABiEAAAE/CAYAAAAg+mBzAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAK2FJREFUeJzt3XuwXWV9N/C1zznJyR0CJkhCIAiKBHCwoECRNoVBtJRRK2AVcFpquTmtikwtF2ulakGh6khhpJ0qTPGClwIdGKVWysUqjWAjd5VwCU0gIZCck9u57f3+0ck7qS/zkrO+69knRz+fv893Pb+z928/z1r7lwOtTqfTqQAAAAAAABrWM9EFAAAAAAAAv5oMIQAAAAAAgCIMIQAAAAAAgCIMIQAAAAAAgCIMIQAAAAAAgCIMIQAAAAAAgCIMIQAAAAAAgCIMIQAAAAAAgCL6duSH2u12tWrVqmr27NlVq9UqXRM7sU6nUw0ODlYLFiyoenrKzrD0Hdt0q+/0HNvTd3SbM5aJYK+j2+x1TAR7HRNB39Ftzlgmwo723Q4NIVatWlUtWrSoseKY/FauXFnttddeRdfQd/yy0n2n53gp+o5uc8YyEex1dJu9jolgr2Mi6Du6zRnLRHi5vtuhIcTs2bOrqqqqM844o5o6dWqtQl544YVaue09+OCDUb7dbsc1HHfccVH+7rvvjmvYb7/9ovwuu+xSOzsyMlLdeOON/7cnStq2xllnnVW7755++um4jiuvvDLKr127Nq7hlFNOifKbNm2KazjiiCOi/IIFC2pnh4eHqxtuuKF43227/qWXXlpNmzat1jXWrFkT17F06dIoP3PmzLiG+++/P8o/++yzcQ1DQ0NR/hWveEWU37p1a/WpT32qa3132GGHVX19O3Qs/z82b94c15HuE/Pnz49rSO27777xNV7zmtdE+Q0bNtTODg0NVddcc01Xz9iTTz65mjJlSq1rnHnmmXEdg4ODUX7FihVxDd/5znei/JNPPhnXsHjx4iif3tvdcsstXdvrkp5r4n7mLW95S5S/9dZb4xrSaxx55JFxDekZOWPGjNrZkZGR6p//+Z+7utedeuqptfuuiXv5o48+Oso//vjjcQ1vetObovzhhx8e15DudWeddVbt7MjISHXzzTd3ba87/fTTaz/DPvfcc3Edddfe5h3veEdcQ6fTifLf+MY34hqGh4ej/O677x7lR0ZGqm9+85td67uLLrqo9nPskiVL4jrS76qaeIZM++6QQw6Ja7j33nujfPIsPjQ0VF155ZVdPWOPOOKI2s+xZ599dlzHfffdF+WfeOKJuIaHH344yjfxfXXyPFBVVdXf3187Ozo6Wv34xz9+2b7boS7Z9mc1U6dOrX2Y1b3p215vb2+Ub+LPg9LDPP0dqip/LdPfoaqaeS13dI2pU6fW/jA00Xfp5r1ly5a4hvTP6Jp4v34d+m7b9adNm1ZNnz691jWSjXubdIgwa9asuIa6v/82TbwOqbo34L+sW33X19dX++atibMl3Wfq1t6kJvaZtG+2bt0a19DNM3bKlCm1X7cmBp7pDXe6V1VV3rs7w71dE/c73drrkp5Lv0yqqrxnmnitU03st78OPbf9GknfNfG7pmdLE+db2vtNfKE1Z86cKD8Z+m5n+e4kvUYyaNwm/TK4idchraGJz15Vdfc5tu5+08R7nu4T6T9Oqar8PU/3qarK75GbeI7t5hmbPMc20Xfpdw87w/fVTUhraOL+8uX6zv+YGgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKMIQAgAAAAAAKKJvPD88NjZWjY2N1VroxRdfrJXb3sjISJT/xCc+Edfwla98JcqvW7curmGfffaJrzGZjIyMVD099eZlX/ziF+P1165dG+U//elPxzVceeWVUf7hhx+Oa7jzzjvja0wWY2Nj1ejoaK3se9/73nj9559/Psq/4Q1viGv40pe+FOWnT58e1zAwMBBfYzIZGxurWq1WrezmzZvj9efMmRPlX/nKV8Y1DA4ORvkmXocjjjgiyv/O7/xO7ezAwED1uc99Llp/vJJ7u8cffzxePz2fli1bFtdwxhlnRPkLLrggrmGvvfaKrzFZtNvtqt1u18o28RlP+7aJvW7evHlRfuPGjXENc+fOja8xmYyOjtZ+nkj29W0WLlwY5ZvYI1auXBnl//Zv/zauIX2e73Q6E5KtI+m59Fmgqqpq6dKlUf4b3/hGXMNTTz0V5d/+9rfHNXzve9+LrzGZjI6O1n6O3bp1a7z+c889F+Wb6Lvly5dH+f7+/riG9Hli6tSpcQ3d1Ol0au+xP/jBD+L116xZE+WbeL2PPPLIKH/vvffGNdS9v+4mfwkBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAU0TeeH261WlWr1aq10NDQUK3c9s4444wo//zzz8c1PPLII1F+ypQpcQ3tdjvK9/TUnz0l2bqSvktfq6qqqt7e3ij/T//0T3ENb3jDG6L8+eefH9dwxx13RPm672Garbte3TU/+clPxuvfeuutUf6EE06Ia0iv8eEPfziuYbfddovyad90u+8SY2Nj8TXmzZsX5detWxfX8OCDD0b5Aw44IK5h+fLlUf7CCy+snR0dHY3WriPZ76666qp4/XvvvTfKn3vuuXEN6eu+M9zbTaYzNnH22WfH13jggQei/CGHHBLX8Nhjj0X5gYGBuIZOpxNfYzJJ9rom7mm+8IUvRPlvfetbcQ0PPfRQlN+6dWtcw6GHHhrlFy1aVDs7mZ4nmnitn3rqqSif9ktVVdX+++8f5XfZZZe4hvQeebI9TyR9lz57VVVVPfPMM1E+/b6tqqpq+vTpUX54eDiuIX0uWrBgQe3sZLqvq6qquuuuu+JrrFixIsrvvvvucQ3HHHNMlJ85c2ZcQxPfCZTmLyEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAiDCEAAAAAAIAi+sbzw61Wq2q1WrUWGhsbq5Xb3n777RflH3jggbiGww47LMr39ORzn9HR0Sjf1zeut/1/abfb0dp1JH138cUXx+ufd955Uf6rX/1qXMNBBx0U5WfNmhXX8OY3vznKr1mzJq6hW5KeGxkZidc/+OCDo/y1114b1/DEE09E+WeffTauIe3bKVOmRPkmzq3x6OnpqX1GTJs2LV5/zz33jPK33357XEPqwAMPjK/x9a9/PcrX3TvS7ERo4jOyxx57RPkDDjggriHt3U6nE9eQnh2T6d4uOWNvvfXWeP30/Zo6dWpcQxPPA6n085v8DhOx1yV9d/fdd8fr77rrrlH+0EMPjWu48847o/zMmTPjGtK9Lrm3a2Kv7pb0eb+qquqGG26I8k2cDQsXLozyw8PDcQ0Teb5W1eQ6Y2fMmBGvv++++0b5P/mTP4lruOWWW6L8M888E9cwkd/ZpT1bR9J3W7Zsidfv7++P8oODg3ENK1eujPLpdxdVlfddcm+3o9mJvwMGAAAAAAB+JRlCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARRhCAAAAAAAARfSN54dbrVbVarVqLTR16tRaue3NmjUryr/xjW+Ma3j88cej/P777x/X8Na3vjXKf/3rX6+d7enp/tyqt7e36u3trZW9//774/UvueSSKD99+vS4hmuuuSbKf/Ob34xrSD9/L7zwQu3s2NhYtPZ4tVqt2r0+bdq0eP2RkZEo/5WvfCWu4XWve12U/8AHPhDX8NWvfjXK1903tun2fpecsenvWlVVddddd0X5DRs2xDUcddRRUX7ZsmVxDevXr4/yCxcurJ3tdDrR2nX09PTU7vUm7u1OPvnkKJ/ul1VVVbfcckuU7+sb1+30S9q8eXOUT/arybTXrVy5Ml7/+eefj/Lz5s2La1iwYEGUf+655+IahoeHo/xk6rlta9Zd9/zzz4/XP+2006L8McccE9cwd+7cKH/ZZZfFNQwODkb55H6niXul8a5Xd80mnifS9zu9H6qqqlq6dGmUv+222+Ia0p5Lz/h2ux3lxyvZ62bPnh2vv3z58iifvl9VVVWnn356lP/oRz8a17Bx48YoP5n2ulQT9wT9/f0Tmq+qfM/9xS9+EdeQnh1178/Hk/WXEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBF94/nhsbGxamxsrNZC06ZNq5Xb3ujoaJT/t3/7t7iG++67L8r/+7//e1zDueeeG+WT1zF9D+oYGRmpWq1Wrezs2bPj9e++++4oPzQ0FNdw7LHHRvlZs2bFNUyZMiXKb926tXZ2eHg4Wnu8xsbGavf6zJkz4/XT9+v222+Pa7j55puj/LXXXhvX8K53vSvKP/HEE1F+ZGQkyo/X2NhY7b2uic/I5s2bo/w+++wT17D77rtH+R/+8IdxDfPmzYvyde+T0myyZt11FyxYEK9/6KGHRvm77rorriHtu40bN8Y1zJkzJ8one0C397p2u1212+1a2fRepKqqqr+/P8pv2rQpruHVr351lP/JT34S15CabHvd6Oho1dNT79/fNfEc+5nPfCbKv+Y1r4lruPjii6P8AQccENeQPktOpr0ueYbdGZ4n9tprr7iGN73pTVH+sssui2vYd999o3z6LN/t59jR0dHan7OrrroqXv/pp5+O8osXL45rOProo6N8+ixQVVXt82abpO+a+P5pvDqdTtXpdGpl+/rG9bX0S0rfs4MPPjiuobe3N8ofddRRcQ3p8/y6detqZ3f03s5fQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEUYQgAAAAAAAEX0jeeHe3p6qp6eenOLadOm1cpt75577onyu+++e1zDk08+GeWPP/74uIZ99tknyp9//vm1sxs3bqyuu+66aP3x6unpqXp7e2tlZ8+eHa+/5557Rvm99947ruH666+P8k28Z3/3d38X5Q8//PDa2eHh4Wjt8Ur2uhkzZsTrX3755VH+3e9+d1zDbrvtFuWfe+65uIY99tgjyj/00ENRfuvWrVF+vJK+6+sb13H+krZs2RLlDzjggLiGp556Kr5Gqu57sM3o6Gjt7NjYWLR2Ha1Wq2q1WrWyH/7wh+P1165dG+V/+tOfxjUsWbIkyh9xxBFxDaecckqUv/TSS2tnR0ZGorXHK+m5Jva6Qw45JMrvsssucQ3pc9HSpUvjGureW2+TnPMTsddN9L3dgQceGOX333//uIb0fnrDhg1xDXPmzInyyb1Zt/e6pOemT58erz80NBTlP/7xj8c1vPDCC1H+7LPPjmtYtmxZlE+fBybTGTt16tR4/fScPu644+Iabr/99ii/bt26uIZ58+ZF+WS/7nbPpf7wD/8wvsajjz4a5Zt47k/33PXr18c1DAwMRPlOp1M72263d+jn/CUEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQhCEEAAAAAABQRN94frjValWtVqvWQv39/bVy2/vud78b5V/5ylfGNXzyk5+M8q973eviGq6++uoof++999bODg8PR2vXkfTdjBkz4vUfffTRKP+f//mfcQ0XXnhhlL/55pvjGtLerfseptlurzlt2rR47UMPPTTKf/vb345rWLduXZR/8skn4xqWLVsW5ffYY48o39Mzeeb0fX3jOs5f0sKFCxuoJPOjH/0oys+bNy+uoYnP8GSSnLFHHnlkvP5//Md/RPk/+qM/imtot9tR/sUXX4xr+NjHPhble3t74xomgyb2uv322y/KDw4OxjUMDAxE+QMPPDCuYenSpVF+yZIltbMDAwPVt771rWj98Zro59hjjz02yu+yyy5xDVdddVWU37hxY1zDPvvsE+Un0/PERPfcW97ylii/efPmuIYrrrgiyp9zzjlxDZdffnmUf8973hPX0E1J31166aXx+rNmzYryl1xySVxD+p3dCSecENcwZ86cKD+Z9rpta9Zd9wMf+EC8/p/+6Z9G+fQ7v6rKn2MXL14c17DbbrtF+W48B0+eb1gAAAAAAIBJxRACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAoom9HfqjT6VRVVVXDw8O1FxoZGamd3WZsbCzKj46OxjVs3bo1ym/evDmuYcuWLVG+ifdxW0+UtLP03cDAQJTfuHFjXEPyGlRVVbXb7biG9PMzGfpu2/WTz/nQ0FBcx87Qc5s2bYry6T5VVflrme7X29bvVt8lZ1x6PjZxjSb221QTe136OjTxPnbzjE3et3Svqqp8r0k/51WV900T+376+Ul+h26fscnv2sQ+09TZkEh7rokzNr1XSD7/27KTZa/bGfpu6tSpcQ0Tuc9skz5PNPE+dmuvm+hn2PT5sYnvLdJ7qib2uvReJX0vJtNzbBP3dTvDPVWqie8N089f8jp06xl2+zWS16yJvktf7ybe89TO8BybvA47+hzb6uxAZz7zzDPVokWLahfDr56VK1dWe+21V9E19B2/rHTf6Tleir6j25yxTAR7Hd1mr2Mi2OuYCPqObnPGMhFeru92aAjRbrerVatWVbNnz65arVajBTK5dDqdanBwsFqwYEHV01P2v+al79imW32n59ievqPbnLFMBHsd3WavYyLY65gI+o5uc8YyEXa073ZoCAEAAAAAADBe/sfUAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEYYQAAAAAABAEX078kPtdrtatWpVNXv27KrVapWuiZ1Yp9OpBgcHqwULFlQ9PWVnWPqObbrVd3qO7ek7us0Zy0Sw19Ft9jomgr2OiaDv6DZnLBNhR/tuh4YQq1atqhYtWtRYcUx+K1eurPbaa6+ia+g7flnpvtNzvBR9R7c5Y5kI9jq6zV7HRLDXMRH0Hd3mjGUivFzf7dAQYvbs2Y0VNJFmzZoVX2Pjxo0NVDL5daMntq3R29tbe6p68MEHx3V87Wtfi/I/+clP4hr+9V//Nco/+eSTcQ0rVqyY8BpK910T129in/nRj34U5YeHh+MaPvWpT0X5W265Ja6hid+jCd3qu/7+/tp73dDQUFxH+q9X5syZE9ewzz77RPk777wzriF9HQYGBmpnBwcHqyVLlnT1jE2sXr06vsb06dOj/H/913/FNbztbW+L8ps3b45rGBsbi/LtdjuuYTKcsU3sMxdddFGUv+mmm+Ia0nP+V8Vk2esuuOCC+BpHH310lL/xxhvjGg466KAo/653vSuu4Z577onyH/zgB2tnO51ONTAw0LW9bo899qj9r5C3bt0a13HaaadF+VNPPXXCa3j++efjGtLzdbfddovy7Xa7WrNmzaQ4Y5977rn4GtOmTYvy3/nOd+IaHn/88Sh/4oknxjWkzyR//ud/Xjvb6XSqoaGhSXPGbtiwoYFKMhdeeGF8jWuvvTbKz58/P65hzZo1UX50dDSu4eV6YoeGEE38Wc3O8Kc5O8Pv0el04hp2Bt14P7et0Wq1aq/X29sb15FurDNmzIhrmDp1apTv69uhj/r/V/qnfEnPbPvclO67nWGPqKq855r48n7KlClRfmfY85vSrb5L9rqdoXebqCHds5v4gnJn6N1unrGJJl7vdAjRxOB3Z+j9X4e+2/76E7nXpV+QNHFP9atgMtzX/fIaddfr7++P65g5c2aUT58Fqirv/Sa+bEqfiybDfrvt+j09PbWfn5r4T6ikfdvE+TqRz49NXaOp/5zNZHiObeK+Lt1nmvjuZGfY69L7252h97u1RhN9l2rinN8Z9prJ8Dzhf0wNAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAU0dethaZMmRJfY3R0NMrfdtttcQ2//du/HeU7nU5cw6+badOmVa1Wq1b2ox/9aLz+ggULovw73/nOCa/hC1/4QlzDeeedF+WnTp1aO9vpdKrh4eFo/fFotVq1e+7Tn/50vP7ee+8d5Xt7e+MaDj744Ch/xRVXxDV88IMfjPJjY2NxDd20devWiS4hMnPmzPgay5cvj/J///d/H9dw1llnRfmNGzfWzm7atClau47+/v7a+11/f3+8/po1a6L8n/3Zn8U1XHPNNVH+tNNOi2tI7w/rvodNrN1N73jHO+JrzJo1K8o38Xr19GT/Dqzdbsc1JD1TVZOrb6rqf+6t6r7u55xzTrz+yMhIlD/88MPjGtL98h/+4R/iGtJ71MHBwdrZbvfs6tWra2fT+/Cqqqp58+ZF+be+9a1xDYsXL47yV199dVzD+973viiffgfVxH49HgceeGDtz1m6T1VV9hmtqqr667/+67iGZcuWRfnLL788rmFoaCjKJ+/FRJzPyfcnTTyzr1+/Psr/8Ic/jGu45557ovzRRx8d15DuV914nvCXEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBGGEAAAAAAAQBF94/nh/v7+qtVq1Vpo6tSptXLbe/3rXx/lm6gh1dOTz33qvgfbdDqdKJvk6+jr66v9Oz/77LPx+ieffHKUHx0djWuYP39+lH/iiSfiGtatWxflk9eh2z2366671u65z372s/H6559/fpRv4vX6yEc+EuUvvPDCuIaxsbH4GpNJq9Wq3XdNnC1f+9rXovzpp58e15C+57NmzYprSD399NO1s5s2bWqwkh3zqle9qurt7a2V/cEPfhCvf+KJJ0b5Y445Jq7h8MMPj/LtdjuugR3z4x//OL7G3Llzo3zas1WV35etXr06riG9V0ieR7p9X1dVVfXMM8/Urvnhhx+O13/zm98c5f/iL/4iriG1Zs2a+BpDQ0NRPrnf6XQ6Xd2vZ8+eXbvnBgcH4/U/8YlPRPkm7kle/epXR/nFixfHNaxfvz7Kp/fY3d7vXnzxxdo1/+xnP4vXT+/l77vvvriG1PPPPx9fI33fJ9u9ZU9PT+397rvf/W68/kknnRTlp0yZEtewxx57RPmd4buPbuxX/hICAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAowhACAAAAAAAoom88Pzw0NFR/ob5xLfWSfvM3fzPK//Ef/3FcQ7vdjvIzZ86Ma5gxY0aUX7t2bVxDN3U6ndrZz372s/H6CxYsiPK/9Vu/Fddw0EEHRfnbbrstrmHatGlRfvr06bWznU6nWr9+fbT+eKxfv75qtVq1si+88EK8fk9PNh++/vrr4xoGBgai/OrVq+Ma6r4H26SvY6fTiff88a5X19y5c+P1L7300ig/MjIS19Db2xvlly5dGtfw85//PMofe+yxtbNJD9T1yCOP1M4ed9xx8fqjo6NR/nd/93fjGk466aT4Gql0v0tMRN/VXfOhhx6K137iiSei/OLFi+MahoeH42tMtLRnJ6Lv6vqN3/iN+BpXX311lD/00EPjGh577LEo38Q+lfb+lClTamc7nU585oxHf39/7XvRp59+Ol4/fb/6+/vjGi688MIo/+UvfzmuYe+9947yv/jFL6J8t/e65557rvZ7n37vUVVV9ZnPfCbKz58/P64h+d6yqvLn4KrK3/ddd901WnvDhg3R+uM1NjZWO/sHf/AH8frpM/u+++4b13DNNdfE10hNhucJfwkBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAUYQgBAAAAAAAU0TfeQKvVqrVQb29vrdz23va2t0X53//9349rOOaYY6L8nDlz4hpe+9rXRvm77767drbT6VRjY2PR+uO1devW2n332GOPxeuvWLEiytetfXv//d//HeWnTJkS1/D6178+yv/85z+vnW2329X69euj9cej0+lUnU6na+v9sna7HeUvuOCCuIYXX3wxyo+MjMQ19PWN+4j6X9LXsds90N/fX3u/2LBhQ7z+wMBAlN91113jGk455ZQof91118U1XHzxxfE16prIfaeO0dHRiS6hetWrXhVf4/HHH2+gkkz63jdxrzEZNPEZ2bx5c5SfP39+XMMb3/jGKP+lL30priGVnrHd1mq1an9OTjrppHj9PffcM8ovXLgwrmHJkiVR/l/+5V/iGtL7w0WLFtXOttvt6sknn4zWH+96dTWx16XXaOL7m3Sve/e73x3XsHHjxig/2c7X5H2/6KKL4vXT57fjjz8+ruHRRx+N8o888khcw+DgYJQ/7LDDamdHR0erO++8M1q/m9LXqgn77rtvfI1bb701yu8Mz4Hd2O/8JQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFBE33h+uNVqVa1Wq9ZCW7ZsqZXbXn9/f5S/6aab4hqGhoaifF/fuF7yl/T9738/yp9wwgm1s6Ojo/H649XT01O779KeqaqqGh4ejvI9Pfmsb8OGDVH+Yx/7WFzD5z//+Si/evXq2tlOpxOtPdnU7fdt1q5dG9eQvuZ77rlnXMMee+wR5R944IG4hm72Xnq+pNK+O+igg+IaPve5z0X53XffPa4hfc+Tc77T6VRjY2PR+pNNekbecccdcQ3paz5lypS4ho985CNR/otf/GLtbLvdrtatWxetP5mk9+If//jH4xoee+yxKH/99dfHNUybNi3Kt9vt2tlOp1Nt3bo1Wn+8kufY5cuXx+v/9Kc/jfJ/9Vd/FdewdOnSKH/LLbfENaT3GitWrIhr6JZZs2bVPuOa2JMXLVoU5efOnRvX8IpXvCLKN3FPlPbcZJPsdTfffHPD1YzfAQccEF9jzpw5Ub6J75DS57rk/vbX7buTJpx77rnxNR588MEo/5d/+ZdxDcm9WVV1p3f8JQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFCEIQQAAAAAAFBE33h+ePbs2VWr1aq10OjoaK3c9j7/+c9H+T333DOuoa9vXC/Z/+O8886La0gtWbKkdnZoaKj6/ve/32A1L29kZKR23y1cuDBe/7DDDovy3/ve9+IaTjjhhCj/6KOPxjW02+0o3+l0JiRbR6vVqt1zu+22W7z+OeecE+Uvu+yyuIb0/T7kkEPiGj70oQ9F+be//e1RvtPpVMPDw9E1uqVuv24v/ZwtXrw4ruHGG2+M8ps3b45rSDVxv9NNPT09tftn7dq18frTp0+P8kcddVRcQ3pvl57RVVVVZ555ZpT/m7/5m9rZbp+xVVV/z+rpyf/9VHpf18R+e+qpp0b5Z599Nq4h7bmnnnqqdnbTpk3V8ccfH60/XhO9N6d9k/ZMVVXVypUro/zs2bPjGtasWRNfY7JYtWrVhO51q1evjvI33XRTXMO3v/3tKD9z5sy4hve///1R/oYbbojy7XY7fi/Go9Pp1D7XmzjftmzZEuVf+9rXxjU8/PDDUb6Jz1/6LJ3mJ5Mmnt/S79xmzJgR15B+93D99dfHNaxYsSLKd+O7D38JAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFGEIAQAAAAAAFNE3nh8eGBiovVCr1aqd3ea6666L8j09+czl+uuvj/Lvec974hqS96Gqquof//Efa2c7nU60dh3Tp0+v3T8LFy6M17/00kuj/Pvf//64hosuuijKL1++PK4h/QwvWLCgdrbdblerVq2K1h+PpM/Tz2dVVdWHPvShKP97v/d7cQ133HFHlH/wwQfjGt75zndG+SbOncmiid916tSpUf7FF1+Ma3jve98bX4PxabfbtbO9vb3x+tOmTYvyF198cVzD/fffH+XPPPPMuIannnoqyifn1kTc29Vds4m9Lu25Rx99NK5h/vz5Uf6SSy6Ja0itXr26dnZ4eLjBSnZc3f456KCD4rUPO+ywKH/33XfHNbzvfe+L8qOjo3ENv06SvXXu3Lnx+ocffniUX7ZsWVzD+eefH+X333//uIZ169ZF+RNPPDHKDw8PV1/+8peja4xHb29v7b3uZz/7Wbx+cl9ZVVV1zTXXxDWk3xs2oYnvHn9dTJ8+Pb7GSSedFOWvuOKKuIbjjz8+ys+YMSOu4aijjoryd955Z1zDy/HJAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAijCEAAAAAAAAiujbkR/qdDrxQk1cY2eoYfPmzVF+YGAgriG9RvI6bMt24/1sYq3R0dG4jo0bN0b5TZs2xTWkv8fO8Plrt9txtvTvsf31667VRI3pZ7yJntu6dWuUHxkZiWtIX8um8t3su8l8jSbec/5HN8/YRBP3NKn0vqyqqmpoaCjKDw4OxjWk+/ZkuLfbGfapqsrvqbZs2RLXkN5b7gyfveR32Nbv3d7r6q43NjYW1zE8PBzlm+i7ib6vauoaO3sNTeypyXPTNjvDXpe+1jvDZ6+3t7eR9SdD3zVxP5P2bvoMWlW/HvvMzlLDr8rzxM7QdzvDPW4TXu73aHV24Dd95plnqkWLFjVWFJPfypUrq7322qvoGvqOX1a67/QcL0Xf0W3OWCaCvY5us9cxEex1TAR9R7c5Y5kIL9d3OzSEaLfb1apVq6rZs2dXrVar0QKZXDqdTjU4OFgtWLCg6ukp+1/z0nds062+03NsT9/Rbc5YJoK9jm6z1zER7HVMBH1HtzljmQg72nc7NIQAAAAAAAAYL/9jagAAAAAAoAhDCAAAAAAAoAhDCAAAAAAAoAhDCAAAAAAAoAhDCAAAAAAAoAhDCAAAAAAAoAhDCAAAAAAAoIj/AwEzlXdq/E/mAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the encoded images:\n",
      "(360, 32)\n",
      "\n",
      "Shape of the decoded images:\n",
      "(360, 64)\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "# Encode and decode some digits\n",
    "encoded_imgs = encoder.predict(X_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "# Data Plotting\n",
    "# Plot the original and reconstructed images\n",
    "n = 10  # Number of digits to display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(8, 8))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(8, 8))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "\n",
    "# Additional Analysis\n",
    "# Print the shape of the encoded images\n",
    "print(\"\\nShape of the encoded images:\")\n",
    "print(encoded_imgs.shape)\n",
    "\n",
    "# Print the shape of the decoded images\n",
    "print(\"\\nShape of the decoded images:\")\n",
    "print(decoded_imgs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
