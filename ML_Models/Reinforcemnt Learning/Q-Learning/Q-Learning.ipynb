{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Q-Learning Model Theory**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "---\n",
    "\n",
    "## Theory\n",
    "Q-learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for an agent interacting with an environment. It is designed to learn the value of taking a specific action in a specific state, which is represented by a Q-value. Q-learning uses a Q-table to store the Q-values for state-action pairs and iteratively updates these values based on the agent's experiences.\n",
    "\n",
    "The main idea is to:\n",
    "- Initialize a Q-table with arbitrary values.\n",
    "- Use an exploration strategy to interact with the environment.\n",
    "- Update the Q-values based on the received rewards.\n",
    "- Gradually converge to the optimal Q-values that represent the best action-selection policy.\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundation\n",
    "- **Q-value**:\n",
    "  The Q-value \\( Q(s, a) \\) represents the expected cumulative reward of taking action \\( a \\) in state \\( s \\) and following the optimal policy thereafter.\n",
    "\n",
    "- **Bellman Equation**:\n",
    "  The Q-value is updated using the Bellman equation:\n",
    "  $$ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right) $$\n",
    "  where:\n",
    "  - \\( s \\) is the current state.\n",
    "  - \\( a \\) is the current action.\n",
    "  - \\( r \\) is the reward received after taking action \\( a \\).\n",
    "  - \\( s' \\) is the next state.\n",
    "  - \\( \\alpha \\) is the learning rate (0 < \\( \\alpha \\) ≤ 1).\n",
    "  - \\( \\gamma \\) is the discount factor (0 ≤ \\( \\gamma \\) ≤ 1).\n",
    "\n",
    "- **Q-table**:\n",
    "  A table that stores the Q-values for all state-action pairs. The table is updated iteratively based on the agent's experiences.\n",
    "\n",
    "- **Exploration vs. Exploitation**:\n",
    "  The agent balances exploration (trying new actions) and exploitation (using the best-known actions) to learn the optimal policy:\n",
    "  - **Exploration**: Choose a random action with probability \\( \\epsilon \\).\n",
    "  - **Exploitation**: Choose the action with the highest Q-value with probability \\(  1- \\epsilon \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm Steps\n",
    "1. **Initialize Q-table**:\n",
    "   - Initialize the Q-table with arbitrary values for all state-action pairs.\n",
    "\n",
    "2. **Repeat for each episode**:\n",
    "   - Initialize the starting state.\n",
    "\n",
    "3. **Repeat for each step of the episode**:\n",
    "   - Choose an action \\( a \\) based on the exploration strategy (e.g., \\( \\epsilon \\)-greedy).\n",
    "   - Take action \\( a \\), observe the reward \\( r \\) and the next state \\( s' \\).\n",
    "   - Update the Q-value using the Bellman equation.\n",
    "   - Update the current state to the next state \\( s' \\).\n",
    "\n",
    "4. **End of Episode**:\n",
    "   - If the episode ends (e.g., the agent reaches a terminal state), reset the environment.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Parameters\n",
    "- **Learning Rate (\\(\\alpha\\))**: Determines the extent to which new information overrides old information.\n",
    "- **Discount Factor (\\(\\gamma\\))**: Determines the importance of future rewards.\n",
    "- **Exploration Rate (\\(\\epsilon\\))**: Determines the probability of choosing a random action.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages\n",
    "- Simple and easy to implement.\n",
    "- Model-free, does not require knowledge of the environment's dynamics.\n",
    "- Can handle stochastic environments.\n",
    "\n",
    "---\n",
    "\n",
    "## Disadvantages\n",
    "- Can be slow to converge for large state-action spaces.\n",
    "- Requires careful tuning of hyperparameters.\n",
    "- May not perform well in environments with sparse rewards.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Tips\n",
    "- Use **exploration strategies** like \\( \\epsilon \\)-greedy, decayed \\( \\epsilon \\)-greedy, or softmax to balance exploration and exploitation.\n",
    "- Normalize rewards to ensure stable learning.\n",
    "- Use **experience replay** to improve sample efficiency and convergence.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications\n",
    "- Game playing (e.g., chess, Go)\n",
    "- Robotics (e.g., navigation, manipulation)\n",
    "- Autonomous driving\n",
    "- Recommendation systems\n",
    "\n",
    "Q-learning is a powerful reinforcement learning algorithm that helps agents learn optimal policies through interactions with the environment. Its simplicity and effectiveness make it a popular choice for various applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation for Q-learning\n",
    "\n",
    "### 1. Q-values\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the learned Q-values to ensure they converge to the optimal values.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher Q-values indicate better expected rewards for the corresponding state-action pairs.\n",
    "- Compare Q-values across different episodes to observe convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Cumulative Reward\n",
    "\n",
    "**Formula:**\n",
    "$$\n",
    "\\text{Cumulative Reward} = \\sum_{t=0}^T r_t\n",
    "$$\n",
    "where:\n",
    "- \\( r_t \\) is the reward received at time step \\( t \\).\n",
    "- \\( T \\) is the total number of time steps.\n",
    "\n",
    "**Description:**\n",
    "- Measures the total reward accumulated by the agent over an episode.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher cumulative rewards indicate better performance.\n",
    "- Plot cumulative rewards across episodes to observe learning progress.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Convergence\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the convergence of the Q-values and cumulative rewards over time.\n",
    "\n",
    "**Interpretation:**\n",
    "- Convergence indicates that the agent has learned a stable policy.\n",
    "- Plot Q-values and cumulative rewards across episodes to check for convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Policy Evaluation\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the learned policy by observing the actions taken by the agent.\n",
    "\n",
    "**Interpretation:**\n",
    "- A good policy should result in optimal actions being taken in each state.\n",
    "- Compare the learned policy with the optimal policy (if known) to assess performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Exploration vs. Exploitation\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the balance between exploration and exploitation during training.\n",
    "\n",
    "**Interpretation:**\n",
    "- A good balance ensures that the agent explores the environment sufficiently while also exploiting the learned policy.\n",
    "- Plot the exploration rate (\\(\\epsilon\\)) over time to observe the balance.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Learning Rate (\\(\\alpha\\))\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the impact of the learning rate on the agent's performance.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher learning rate may lead to faster convergence but can cause instability.\n",
    "- A lower learning rate may result in slower convergence but more stable learning.\n",
    "- Experiment with different learning rates to find the optimal value.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Discount Factor (\\(\\gamma\\))\n",
    "\n",
    "**Description:**\n",
    "- Evaluate the impact of the discount factor on the agent's performance.\n",
    "\n",
    "**Interpretation:**\n",
    "- A higher discount factor places more importance on future rewards.\n",
    "- A lower discount factor places more importance on immediate rewards.\n",
    "- Experiment with different discount factors to find the optimal value.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Training Time\n",
    "\n",
    "**Description:**\n",
    "- Measure the time taken to train the agent.\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower training times indicate more efficient learning.\n",
    "- Evaluate training time in conjunction with other metrics to assess overall performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Success Rate\n",
    "\n",
    "**Description:**\n",
    "- Measure the success rate of the agent in achieving its goal.\n",
    "\n",
    "**Interpretation:**\n",
    "- Higher success rates indicate better performance.\n",
    "- Plot success rates across episodes to observe learning progress.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Episode Length\n",
    "\n",
    "**Description:**\n",
    "- Measure the average length of episodes.\n",
    "\n",
    "**Interpretation:**\n",
    "- Shorter episodes may indicate better performance (e.g., reaching the goal faster).\n",
    "- Plot episode lengths across episodes to observe learning progress.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "### Custom Implementation\n",
    "\n",
    "| **Parameter**   | **Description**                                                                 |\n",
    "|-----------------|-------------------------------------------------------------------------------|\n",
    "| alpha           | Learning rate (0 < alpha ≤ 1).                                                |\n",
    "| gamma           | Discount factor (0 ≤ gamma ≤ 1).                                              |\n",
    "| epsilon         | Exploration rate (probability of choosing a random action).                   |\n",
    "| epsilon_decay   | Decay rate for the exploration rate.                                          |\n",
    "| num_episodes    | Number of training episodes.                                                  |\n",
    "| max_steps       | Maximum number of steps per episode.                                          |\n",
    "\n",
    "-\n",
    "\n",
    "| **Attribute**           | **Description**                                                                 |\n",
    "|-------------------------|-------------------------------------------------------------------------------|\n",
    "| Q-table                 | A table storing Q-values for state-action pairs.                             |\n",
    "| policy                  | The learned policy (mapping from states to actions).                        |\n",
    "\n",
    "-\n",
    "\n",
    "| **Method**              | **Description**                                                                 |\n",
    "|-------------------------|-------------------------------------------------------------------------------|\n",
    "| choose_action(state)    | Choose an action based on the exploration strategy.                           |\n",
    "| update_q_table(state, action, reward, next_state) | Update the Q-table using the Bellman equation.         |\n",
    "| train(env)              | Train the Q-learning agent in the given environment.                         |\n",
    "| evaluate(env)           | Evaluate the performance of the Q-learning agent in the environment.          |\n",
    "| decay_epsilon()         | Decay the exploration rate over time.                                         |\n",
    "| reset()                 | Reset the Q-learning agent for a new training run.                            |\n",
    "\n",
    "-\n",
    "\n",
    "| **Usage Example (Python)**  | **Code**                                                                 |\n",
    "|-----------------------------|------------------------------------------------------------------------|\n",
    "| **Initialize Q-learning agent** | `agent = QLearningAgent(alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.99, num_episodes=1000, max_steps=100)` |\n",
    "| **Train the agent** | `agent.train(env)` |\n",
    "| **Evaluate the agent** | `agent.evaluate(env)` |\n",
    "| **Choose an action** | `action = agent.choose_action(state)` |\n",
    "| **Update Q-table** | `agent.update_q_table(state, action, reward, next_state)` |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XXXXXXXX regression - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool8'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_table[state, :])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Take action and observe the outcome\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Update Q-table using the Bellman equation\u001b[39;00m\n",
      "File \u001b[0;32m~/Learning/ML_Theory/.venv/lib/python3.12/site-packages/gym/wrappers/time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Learning/ML_Theory/.venv/lib/python3.12/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Learning/ML_Theory/.venv/lib/python3.12/site-packages/gym/wrappers/env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_step_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/Learning/ML_Theory/.venv/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[0;34m(env, action)\u001b[0m\n\u001b[1;32m    230\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# np.bool is actual python bool not np boolean type, therefore bool_ or bool8\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(terminated, (\u001b[38;5;28mbool\u001b[39m, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool8\u001b[49m)):\n\u001b[1;32m    234\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects `terminated` signal to be a boolean, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(terminated)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncated, (\u001b[38;5;28mbool\u001b[39m, np\u001b[38;5;241m.\u001b[39mbool8)):\n",
      "File \u001b[0;32m~/Learning/ML_Theory/.venv/lib/python3.12/site-packages/numpy/__init__.py:410\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchar\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchar\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m char\u001b[38;5;241m.\u001b[39mchararray\n\u001b[0;32m--> 410\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool8'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Loading\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Data Preprocessing\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.99  # Decay rate for exploration\n",
    "num_episodes = 1000  # Number of training episodes\n",
    "max_steps = 100  # Maximum steps per episode\n",
    "\n",
    "# Training variables\n",
    "rewards = []\n",
    "\n",
    "# Model Definition and Training\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_rewards = 0\n",
    "    for step in range(max_steps):\n",
    "        # Choose action (exploration vs. exploitation)\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "\n",
    "        # Take action and observe the outcome\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Update Q-table using the Bellman equation\n",
    "        best_next_action = np.argmax(q_table[next_state, :])\n",
    "        q_table[state, action] += alpha * (reward + gamma * q_table[next_state, best_next_action] - q_table[state, action])\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        total_rewards += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay the exploration rate\n",
    "    epsilon *= epsilon_decay\n",
    "    rewards.append(total_rewards)\n",
    "\n",
    "# Data Plotting\n",
    "plt.plot(range(num_episodes), rewards)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.title('Training Rewards over Episodes')\n",
    "plt.show()\n",
    "\n",
    "# Model Evaluation\n",
    "num_eval_episodes = 100\n",
    "total_eval_rewards = 0\n",
    "\n",
    "for episode in range(num_eval_episodes):\n",
    "    state = env.reset()\n",
    "    episode_rewards = 0\n",
    "    for step in range(max_steps):\n",
    "        action = np.argmax(q_table[state, :])  # Exploitation\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        episode_rewards += reward\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    total_eval_rewards += episode_rewards\n",
    "\n",
    "avg_eval_reward = total_eval_rewards / num_eval_episodes\n",
    "print(f\"Average Evaluation Reward: {avg_eval_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
